{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forminju/DACON_anomaly_detection_/blob/main/0227_1_2___smartfactory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKaLVHk_Usyp",
        "outputId": "9c572d16-78fb-4f40-a295-f1c0b7eb1e21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import psutil\n",
        "import gc\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from umap import UMAP\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "# setting some globl config\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "orange_black = [\n",
        "    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n",
        "]\n",
        "plt.rcParams['figure.figsize'] = (16,9)\n",
        "plt.rcParams[\"figure.facecolor\"] = '#FFFACD'\n",
        "plt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\n",
        "plt.rcParams[\"axes.grid\"] = True\n",
        "plt.rcParams[\"grid.color\"] = orange_black[3]\n",
        "plt.rcParams[\"grid.alpha\"] = 0.5\n",
        "plt.rcParams[\"grid.linestyle\"] = '--'\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "INFERENCE = True"
      ],
      "metadata": {
        "id": "YeIoTCMAeG_-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "1kDNwfHSFkCy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed_value):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        \n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "pQ1prUmDEcxB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/스마트공장"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3OE2mOiUus_",
        "outputId": "44f90c65-f547-4578-9a7a-6c01fd6e67a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/스마트공장\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "s5A3THjzVHEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/스마트공장/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/스마트공장/test.csv')"
      ],
      "metadata": {
        "id": "-tWVg24zVIoT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_df.drop(columns=['TIMESTAMP', 'Y_Class','Y_Quality'])\n",
        "train_y = train_df['Y_Class']\n",
        "\n",
        "test_x = test_df.drop(columns=['TIMESTAMP'])"
      ],
      "metadata": {
        "id": "N86cft8kVC3o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 파생변수 생성"
      ],
      "metadata": {
        "id": "KNjZIHjGilhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "tzitA_0Qinff"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "n3k0XMkIiyNA",
        "outputId": "e5189cb4-ec27-484c-c40a-7bf847603fdf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    PRODUCT_ID     LINE PRODUCT_CODE   X_1   X_2  X_3   X_4   X_5  X_6   X_7  \\\n",
              "0    TRAIN_000  T050304         A_31   NaN   NaN  NaN   NaN   NaN  NaN   NaN   \n",
              "1    TRAIN_001  T050307         A_31   NaN   NaN  NaN   NaN   NaN  NaN   NaN   \n",
              "2    TRAIN_002  T050304         A_31   NaN   NaN  NaN   NaN   NaN  NaN   NaN   \n",
              "3    TRAIN_003  T050307         A_31   NaN   NaN  NaN   NaN   NaN  NaN   NaN   \n",
              "4    TRAIN_004  T050304         A_31   NaN   NaN  NaN   NaN   NaN  NaN   NaN   \n",
              "..         ...      ...          ...   ...   ...  ...   ...   ...  ...   ...   \n",
              "593  TRAIN_593  T100306         T_31   2.0  95.0  0.0  45.0  10.0  0.0  50.0   \n",
              "594  TRAIN_594  T050304         A_31   NaN   NaN  NaN   NaN   NaN  NaN   NaN   \n",
              "595  TRAIN_595  T050304         A_31   NaN   NaN  NaN   NaN   NaN  NaN   NaN   \n",
              "596  TRAIN_596  T100304         O_31  40.0  94.0  0.0  45.0  11.0  0.0  45.0   \n",
              "597  TRAIN_597  T100306         O_31  21.0  87.0  0.0  45.0  10.0  0.0  61.0   \n",
              "\n",
              "     ...  X_2866  X_2867  X_2868  X_2869  X_2870  X_2871  X_2872  X_2873  \\\n",
              "0    ...   39.34   40.89   32.56   34.09   77.77     NaN     NaN     NaN   \n",
              "1    ...   38.89   42.82   43.92   35.34   72.55     NaN     NaN     NaN   \n",
              "2    ...   39.19   36.65   42.47   36.53   78.35     NaN     NaN     NaN   \n",
              "3    ...   37.74   39.17   52.17   30.58   71.78     NaN     NaN     NaN   \n",
              "4    ...   38.70   41.89   46.93   33.09   76.97     NaN     NaN     NaN   \n",
              "..   ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "593  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
              "594  ...   49.47   53.07   50.89   55.10   66.49     1.0     NaN     NaN   \n",
              "595  ...     NaN     NaN     NaN     NaN     NaN     1.0     NaN     NaN   \n",
              "596  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
              "597  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
              "\n",
              "     X_2874  X_2875  \n",
              "0       NaN     NaN  \n",
              "1       NaN     NaN  \n",
              "2       NaN     NaN  \n",
              "3       NaN     NaN  \n",
              "4       NaN     NaN  \n",
              "..      ...     ...  \n",
              "593     NaN     NaN  \n",
              "594     NaN     NaN  \n",
              "595     NaN     NaN  \n",
              "596     NaN     NaN  \n",
              "597     NaN     NaN  \n",
              "\n",
              "[598 rows x 2878 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c892b2b3-dd08-4e13-bed7-6421bf3e8301\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>LINE</th>\n",
              "      <th>PRODUCT_CODE</th>\n",
              "      <th>X_1</th>\n",
              "      <th>X_2</th>\n",
              "      <th>X_3</th>\n",
              "      <th>X_4</th>\n",
              "      <th>X_5</th>\n",
              "      <th>X_6</th>\n",
              "      <th>X_7</th>\n",
              "      <th>...</th>\n",
              "      <th>X_2866</th>\n",
              "      <th>X_2867</th>\n",
              "      <th>X_2868</th>\n",
              "      <th>X_2869</th>\n",
              "      <th>X_2870</th>\n",
              "      <th>X_2871</th>\n",
              "      <th>X_2872</th>\n",
              "      <th>X_2873</th>\n",
              "      <th>X_2874</th>\n",
              "      <th>X_2875</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000</td>\n",
              "      <td>T050304</td>\n",
              "      <td>A_31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>39.34</td>\n",
              "      <td>40.89</td>\n",
              "      <td>32.56</td>\n",
              "      <td>34.09</td>\n",
              "      <td>77.77</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_001</td>\n",
              "      <td>T050307</td>\n",
              "      <td>A_31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>38.89</td>\n",
              "      <td>42.82</td>\n",
              "      <td>43.92</td>\n",
              "      <td>35.34</td>\n",
              "      <td>72.55</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_002</td>\n",
              "      <td>T050304</td>\n",
              "      <td>A_31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>39.19</td>\n",
              "      <td>36.65</td>\n",
              "      <td>42.47</td>\n",
              "      <td>36.53</td>\n",
              "      <td>78.35</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_003</td>\n",
              "      <td>T050307</td>\n",
              "      <td>A_31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>37.74</td>\n",
              "      <td>39.17</td>\n",
              "      <td>52.17</td>\n",
              "      <td>30.58</td>\n",
              "      <td>71.78</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_004</td>\n",
              "      <td>T050304</td>\n",
              "      <td>A_31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>38.70</td>\n",
              "      <td>41.89</td>\n",
              "      <td>46.93</td>\n",
              "      <td>33.09</td>\n",
              "      <td>76.97</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>593</th>\n",
              "      <td>TRAIN_593</td>\n",
              "      <td>T100306</td>\n",
              "      <td>T_31</td>\n",
              "      <td>2.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594</th>\n",
              "      <td>TRAIN_594</td>\n",
              "      <td>T050304</td>\n",
              "      <td>A_31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>49.47</td>\n",
              "      <td>53.07</td>\n",
              "      <td>50.89</td>\n",
              "      <td>55.10</td>\n",
              "      <td>66.49</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>TRAIN_595</td>\n",
              "      <td>T050304</td>\n",
              "      <td>A_31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>TRAIN_596</td>\n",
              "      <td>T100304</td>\n",
              "      <td>O_31</td>\n",
              "      <td>40.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>TRAIN_597</td>\n",
              "      <td>T100306</td>\n",
              "      <td>O_31</td>\n",
              "      <td>21.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>598 rows × 2878 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c892b2b3-dd08-4e13-bed7-6421bf3e8301')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c892b2b3-dd08-4e13-bed7-6421bf3e8301 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c892b2b3-dd08-4e13-bed7-6421bf3e8301');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x['LINE_PRODUCT'] = train_x['LINE'].str.cat(train_x['PRODUCT_CODE'])\n",
        "test_x['LINE_PRODUCT'] = test_x['LINE'].str.cat(test_x['PRODUCT_CODE'])"
      ],
      "metadata": {
        "id": "k2zGNX0LiqKe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# qualitative to quantitative\n",
        "qual_col = ['LINE', 'PRODUCT_CODE', 'LINE_PRODUCT']\n",
        "\n",
        "for i in qual_col:\n",
        "    le = LabelEncoder()\n",
        "    le = le.fit(train_x[i])\n",
        "    train_x[i] = le.transform(train_x[i])\n",
        "    \n",
        "    for label in np.unique(test_x[i]): \n",
        "        if label not in le.classes_: \n",
        "            le.classes_ = np.append(le.classes_, label)\n",
        "    test_x[i] = le.transform(test_x[i]) \n",
        "print('Done.')\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qokg-212i6PY",
        "outputId": "14cd8d0d-63b7-4e89-931e-2824f6b34c6b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.drop(['LINE', 'PRODUCT_CODE'],axis=1, inplace=True)\n",
        "test_x.drop(['LINE', 'PRODUCT_CODE'],axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "Glc36faDi8_x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imputation 기법 적용"
      ],
      "metadata": {
        "id": "fUnMA4bH7FIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_x.dropna(how='all')\n",
        "test_x = test_x[train_x.columns]"
      ],
      "metadata": {
        "id": "VzmzZxuejAIF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = test_x[train_x.columns]"
      ],
      "metadata": {
        "id": "Pmp528_N6T9S"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_x.dropna(how='all',axis=1)\n",
        "test_x = test_x[train_x.columns]"
      ],
      "metadata": {
        "id": "VxyFd7606WGq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=50)\n",
        "\n",
        "imputer_output = imputer.fit_transform(train_x.iloc[:,1:])"
      ],
      "metadata": {
        "id": "3rM7l-Ykeaxw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.iloc[:,1:] = pd.DataFrame(imputer_output, columns=train_x.iloc[:,1:].columns, index=list(train_x.iloc[:,1:].index.values));train_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "UgtrM00R97eL",
        "outputId": "0186bae7-e654-4f3f-966a-6295617614e0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    PRODUCT_ID    X_1    X_2  X_3   X_4    X_5  X_6    X_7    X_8   X_9  ...  \\\n",
              "0    TRAIN_000   1.92  94.40  0.0  45.0  10.44  0.0  48.26  10.02  41.5  ...   \n",
              "1    TRAIN_001   1.92  94.40  0.0  45.0  10.44  0.0  48.26  10.02  41.5  ...   \n",
              "2    TRAIN_002   2.58  97.88  0.0  45.0  10.60  0.0  45.00  10.00  31.0  ...   \n",
              "3    TRAIN_003   2.58  97.88  0.0  45.0  10.60  0.0  45.00  10.00  31.0  ...   \n",
              "4    TRAIN_004   2.58  97.88  0.0  45.0  10.60  0.0  45.00  10.00  31.0  ...   \n",
              "..         ...    ...    ...  ...   ...    ...  ...    ...    ...   ...  ...   \n",
              "593  TRAIN_593   2.00  95.00  0.0  45.0  10.00  0.0  50.00  10.00  52.0  ...   \n",
              "594  TRAIN_594   1.92  94.40  0.0  45.0  10.44  0.0  48.26  10.02  41.5  ...   \n",
              "595  TRAIN_595   1.92  94.40  0.0  45.0  10.44  0.0  48.26  10.02  41.5  ...   \n",
              "596  TRAIN_596  40.00  94.00  0.0  45.0  11.00  0.0  45.00  10.00  31.0  ...   \n",
              "597  TRAIN_597  21.00  87.00  0.0  45.0  10.00  0.0  61.00  10.00  52.0  ...   \n",
              "\n",
              "     X_2863      X_2864  X_2865   X_2866   X_2867   X_2868   X_2869  X_2870  \\\n",
              "0    383.00  368.296296  353.00  39.3400  40.8900  32.5600  34.0900  77.770   \n",
              "1    383.00  367.735849  353.00  38.8900  42.8200  43.9200  35.3400  72.550   \n",
              "2    383.00  367.320755  353.00  39.1900  36.6500  42.4700  36.5300  78.350   \n",
              "3    384.00  369.188679  353.00  37.7400  39.1700  52.1700  30.5800  71.780   \n",
              "4    383.00  367.351852  352.00  38.7000  41.8900  46.9300  33.0900  76.970   \n",
              "..      ...         ...     ...      ...      ...      ...      ...     ...   \n",
              "593  383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.446   \n",
              "594  384.00  369.811321  353.00  49.4700  53.0700  50.8900  55.1000  66.490   \n",
              "595  383.00  367.018868  352.00  47.1100  49.5134  49.2178  47.3692  68.439   \n",
              "596  383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.446   \n",
              "597  383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.446   \n",
              "\n",
              "     X_2871  LINE_PRODUCT  \n",
              "0       1.0           2.0  \n",
              "1       1.0           3.0  \n",
              "2       1.0           2.0  \n",
              "3       1.0           3.0  \n",
              "4       1.0           2.0  \n",
              "..      ...           ...  \n",
              "593     1.0           7.0  \n",
              "594     1.0           2.0  \n",
              "595     1.0           2.0  \n",
              "596     1.0           4.0  \n",
              "597     1.0           6.0  \n",
              "\n",
              "[598 rows x 2795 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a055998e-b27d-4357-8944-6e528d268a58\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>X_1</th>\n",
              "      <th>X_2</th>\n",
              "      <th>X_3</th>\n",
              "      <th>X_4</th>\n",
              "      <th>X_5</th>\n",
              "      <th>X_6</th>\n",
              "      <th>X_7</th>\n",
              "      <th>X_8</th>\n",
              "      <th>X_9</th>\n",
              "      <th>...</th>\n",
              "      <th>X_2863</th>\n",
              "      <th>X_2864</th>\n",
              "      <th>X_2865</th>\n",
              "      <th>X_2866</th>\n",
              "      <th>X_2867</th>\n",
              "      <th>X_2868</th>\n",
              "      <th>X_2869</th>\n",
              "      <th>X_2870</th>\n",
              "      <th>X_2871</th>\n",
              "      <th>LINE_PRODUCT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000</td>\n",
              "      <td>1.92</td>\n",
              "      <td>94.40</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.26</td>\n",
              "      <td>10.02</td>\n",
              "      <td>41.5</td>\n",
              "      <td>...</td>\n",
              "      <td>383.00</td>\n",
              "      <td>368.296296</td>\n",
              "      <td>353.00</td>\n",
              "      <td>39.3400</td>\n",
              "      <td>40.8900</td>\n",
              "      <td>32.5600</td>\n",
              "      <td>34.0900</td>\n",
              "      <td>77.770</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_001</td>\n",
              "      <td>1.92</td>\n",
              "      <td>94.40</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.26</td>\n",
              "      <td>10.02</td>\n",
              "      <td>41.5</td>\n",
              "      <td>...</td>\n",
              "      <td>383.00</td>\n",
              "      <td>367.735849</td>\n",
              "      <td>353.00</td>\n",
              "      <td>38.8900</td>\n",
              "      <td>42.8200</td>\n",
              "      <td>43.9200</td>\n",
              "      <td>35.3400</td>\n",
              "      <td>72.550</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_002</td>\n",
              "      <td>2.58</td>\n",
              "      <td>97.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.00</td>\n",
              "      <td>367.320755</td>\n",
              "      <td>353.00</td>\n",
              "      <td>39.1900</td>\n",
              "      <td>36.6500</td>\n",
              "      <td>42.4700</td>\n",
              "      <td>36.5300</td>\n",
              "      <td>78.350</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_003</td>\n",
              "      <td>2.58</td>\n",
              "      <td>97.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>384.00</td>\n",
              "      <td>369.188679</td>\n",
              "      <td>353.00</td>\n",
              "      <td>37.7400</td>\n",
              "      <td>39.1700</td>\n",
              "      <td>52.1700</td>\n",
              "      <td>30.5800</td>\n",
              "      <td>71.780</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_004</td>\n",
              "      <td>2.58</td>\n",
              "      <td>97.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.00</td>\n",
              "      <td>367.351852</td>\n",
              "      <td>352.00</td>\n",
              "      <td>38.7000</td>\n",
              "      <td>41.8900</td>\n",
              "      <td>46.9300</td>\n",
              "      <td>33.0900</td>\n",
              "      <td>76.970</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>593</th>\n",
              "      <td>TRAIN_593</td>\n",
              "      <td>2.00</td>\n",
              "      <td>95.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>52.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.446</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594</th>\n",
              "      <td>TRAIN_594</td>\n",
              "      <td>1.92</td>\n",
              "      <td>94.40</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.26</td>\n",
              "      <td>10.02</td>\n",
              "      <td>41.5</td>\n",
              "      <td>...</td>\n",
              "      <td>384.00</td>\n",
              "      <td>369.811321</td>\n",
              "      <td>353.00</td>\n",
              "      <td>49.4700</td>\n",
              "      <td>53.0700</td>\n",
              "      <td>50.8900</td>\n",
              "      <td>55.1000</td>\n",
              "      <td>66.490</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>TRAIN_595</td>\n",
              "      <td>1.92</td>\n",
              "      <td>94.40</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.26</td>\n",
              "      <td>10.02</td>\n",
              "      <td>41.5</td>\n",
              "      <td>...</td>\n",
              "      <td>383.00</td>\n",
              "      <td>367.018868</td>\n",
              "      <td>352.00</td>\n",
              "      <td>47.1100</td>\n",
              "      <td>49.5134</td>\n",
              "      <td>49.2178</td>\n",
              "      <td>47.3692</td>\n",
              "      <td>68.439</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>TRAIN_596</td>\n",
              "      <td>40.00</td>\n",
              "      <td>94.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>11.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.446</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>TRAIN_597</td>\n",
              "      <td>21.00</td>\n",
              "      <td>87.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>61.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>52.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.446</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>598 rows × 2795 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a055998e-b27d-4357-8944-6e528d268a58')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a055998e-b27d-4357-8944-6e528d268a58 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a055998e-b27d-4357-8944-6e528d268a58');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = test_x[train_x.columns]"
      ],
      "metadata": {
        "id": "x--pOUPcc5gw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer_output_test = imputer.transform(test_x.iloc[:,1:])"
      ],
      "metadata": {
        "id": "sYzMOWZK-PmD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x.iloc[:,1:] = pd.DataFrame(imputer_output_test, columns=test_x.iloc[:,1:].columns, index=list(test_x.iloc[:,1:].index.values));test_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "qpBjxK7B-Skx",
        "outputId": "9647d1bc-a328-4f5c-b5da-e64ce188c352"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    PRODUCT_ID   X_1    X_2  X_3   X_4    X_5  X_6   X_7   X_8   X_9  ...  \\\n",
              "0     TEST_000  2.00  94.00  0.0  45.0  10.00  0.0  51.0  10.0  52.0  ...   \n",
              "1     TEST_001  2.00  93.00  0.0  45.0  11.00  0.0  45.0  10.0  31.0  ...   \n",
              "2     TEST_002  2.00  95.00  0.0  45.0  11.00  0.0  45.0  10.0  31.0  ...   \n",
              "3     TEST_003  4.78  98.46  0.0  45.0  10.84  0.0  45.0  10.0  31.0  ...   \n",
              "4     TEST_004  4.78  98.46  0.0  45.0  10.84  0.0  45.0  10.0  31.0  ...   \n",
              "..         ...   ...    ...  ...   ...    ...  ...   ...   ...   ...  ...   \n",
              "305   TEST_305  2.00  91.00  0.0  45.0  10.00  0.0  51.0  10.0  52.0  ...   \n",
              "306   TEST_306  2.00  96.00  0.0  45.0  11.00  0.0  45.0  10.0  31.0  ...   \n",
              "307   TEST_307  2.00  91.00  0.0  45.0  10.00  0.0  50.0  10.0  52.0  ...   \n",
              "308   TEST_308  2.00  95.00  0.0  45.0  10.00  0.0  51.0  10.0  52.0  ...   \n",
              "309   TEST_309  2.00  87.00  0.0  45.0  10.00  0.0  51.0  10.0  52.0  ...   \n",
              "\n",
              "     X_2863      X_2864  X_2865   X_2866   X_2867   X_2868   X_2869   X_2870  \\\n",
              "0    383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.4460   \n",
              "1    383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.4460   \n",
              "2    383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.4460   \n",
              "3    467.00  444.192308  423.00  46.9280  49.4274  49.0986  47.4816  68.4868   \n",
              "4    465.00  443.211539  423.00  46.9280  49.4274  49.0986  47.4816  68.4868   \n",
              "..      ...         ...     ...      ...      ...      ...      ...      ...   \n",
              "305  383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.4460   \n",
              "306  383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.4460   \n",
              "307  383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.4460   \n",
              "308  383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.4460   \n",
              "309  383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.4460   \n",
              "\n",
              "     X_2871  LINE_PRODUCT  \n",
              "0       1.0           7.0  \n",
              "1       1.0           5.0  \n",
              "2       1.0           5.0  \n",
              "3       1.0           0.0  \n",
              "4       1.0           1.0  \n",
              "..      ...           ...  \n",
              "305     1.0           7.0  \n",
              "306     1.0           5.0  \n",
              "307     1.0           7.0  \n",
              "308     1.0           7.0  \n",
              "309     1.0           7.0  \n",
              "\n",
              "[310 rows x 2795 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-06f5228b-be27-4c8c-9381-6bf3e465602b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>X_1</th>\n",
              "      <th>X_2</th>\n",
              "      <th>X_3</th>\n",
              "      <th>X_4</th>\n",
              "      <th>X_5</th>\n",
              "      <th>X_6</th>\n",
              "      <th>X_7</th>\n",
              "      <th>X_8</th>\n",
              "      <th>X_9</th>\n",
              "      <th>...</th>\n",
              "      <th>X_2863</th>\n",
              "      <th>X_2864</th>\n",
              "      <th>X_2865</th>\n",
              "      <th>X_2866</th>\n",
              "      <th>X_2867</th>\n",
              "      <th>X_2868</th>\n",
              "      <th>X_2869</th>\n",
              "      <th>X_2870</th>\n",
              "      <th>X_2871</th>\n",
              "      <th>LINE_PRODUCT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000</td>\n",
              "      <td>2.00</td>\n",
              "      <td>94.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.4460</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_001</td>\n",
              "      <td>2.00</td>\n",
              "      <td>93.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>11.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.4460</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_002</td>\n",
              "      <td>2.00</td>\n",
              "      <td>95.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>11.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.4460</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_003</td>\n",
              "      <td>4.78</td>\n",
              "      <td>98.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.84</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>467.00</td>\n",
              "      <td>444.192308</td>\n",
              "      <td>423.00</td>\n",
              "      <td>46.9280</td>\n",
              "      <td>49.4274</td>\n",
              "      <td>49.0986</td>\n",
              "      <td>47.4816</td>\n",
              "      <td>68.4868</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_004</td>\n",
              "      <td>4.78</td>\n",
              "      <td>98.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.84</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>465.00</td>\n",
              "      <td>443.211539</td>\n",
              "      <td>423.00</td>\n",
              "      <td>46.9280</td>\n",
              "      <td>49.4274</td>\n",
              "      <td>49.0986</td>\n",
              "      <td>47.4816</td>\n",
              "      <td>68.4868</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>TEST_305</td>\n",
              "      <td>2.00</td>\n",
              "      <td>91.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.4460</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>TEST_306</td>\n",
              "      <td>2.00</td>\n",
              "      <td>96.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>11.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.4460</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>TEST_307</td>\n",
              "      <td>2.00</td>\n",
              "      <td>91.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.4460</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>TEST_308</td>\n",
              "      <td>2.00</td>\n",
              "      <td>95.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.4460</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>TEST_309</td>\n",
              "      <td>2.00</td>\n",
              "      <td>87.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.4460</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>310 rows × 2795 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06f5228b-be27-4c8c-9381-6bf3e465602b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-06f5228b-be27-4c8c-9381-6bf3e465602b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-06f5228b-be27-4c8c-9381-6bf3e465602b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "FZstEws0ox5T",
        "outputId": "07bcf660-2f01-4951-ecf6-3afe34ef023f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    PRODUCT_ID    X_1    X_2  X_3   X_4    X_5  X_6    X_7    X_8   X_9  ...  \\\n",
              "0    TRAIN_000   1.92  94.40  0.0  45.0  10.44  0.0  48.26  10.02  41.5  ...   \n",
              "1    TRAIN_001   1.92  94.40  0.0  45.0  10.44  0.0  48.26  10.02  41.5  ...   \n",
              "2    TRAIN_002   2.58  97.88  0.0  45.0  10.60  0.0  45.00  10.00  31.0  ...   \n",
              "3    TRAIN_003   2.58  97.88  0.0  45.0  10.60  0.0  45.00  10.00  31.0  ...   \n",
              "4    TRAIN_004   2.58  97.88  0.0  45.0  10.60  0.0  45.00  10.00  31.0  ...   \n",
              "..         ...    ...    ...  ...   ...    ...  ...    ...    ...   ...  ...   \n",
              "593  TRAIN_593   2.00  95.00  0.0  45.0  10.00  0.0  50.00  10.00  52.0  ...   \n",
              "594  TRAIN_594   1.92  94.40  0.0  45.0  10.44  0.0  48.26  10.02  41.5  ...   \n",
              "595  TRAIN_595   1.92  94.40  0.0  45.0  10.44  0.0  48.26  10.02  41.5  ...   \n",
              "596  TRAIN_596  40.00  94.00  0.0  45.0  11.00  0.0  45.00  10.00  31.0  ...   \n",
              "597  TRAIN_597  21.00  87.00  0.0  45.0  10.00  0.0  61.00  10.00  52.0  ...   \n",
              "\n",
              "     X_2863      X_2864  X_2865   X_2866   X_2867   X_2868   X_2869  X_2870  \\\n",
              "0    383.00  368.296296  353.00  39.3400  40.8900  32.5600  34.0900  77.770   \n",
              "1    383.00  367.735849  353.00  38.8900  42.8200  43.9200  35.3400  72.550   \n",
              "2    383.00  367.320755  353.00  39.1900  36.6500  42.4700  36.5300  78.350   \n",
              "3    384.00  369.188679  353.00  37.7400  39.1700  52.1700  30.5800  71.780   \n",
              "4    383.00  367.351852  352.00  38.7000  41.8900  46.9300  33.0900  76.970   \n",
              "..      ...         ...     ...      ...      ...      ...      ...     ...   \n",
              "593  383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.446   \n",
              "594  384.00  369.811321  353.00  49.4700  53.0700  50.8900  55.1000  66.490   \n",
              "595  383.00  367.018868  352.00  47.1100  49.5134  49.2178  47.3692  68.439   \n",
              "596  383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.446   \n",
              "597  383.82  367.359196  350.78  52.1678  55.9140  49.1138  53.4380  65.446   \n",
              "\n",
              "     X_2871  LINE_PRODUCT  \n",
              "0       1.0           2.0  \n",
              "1       1.0           3.0  \n",
              "2       1.0           2.0  \n",
              "3       1.0           3.0  \n",
              "4       1.0           2.0  \n",
              "..      ...           ...  \n",
              "593     1.0           7.0  \n",
              "594     1.0           2.0  \n",
              "595     1.0           2.0  \n",
              "596     1.0           4.0  \n",
              "597     1.0           6.0  \n",
              "\n",
              "[598 rows x 2795 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b690bab-3254-4eb0-bb37-afe1a07534d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>X_1</th>\n",
              "      <th>X_2</th>\n",
              "      <th>X_3</th>\n",
              "      <th>X_4</th>\n",
              "      <th>X_5</th>\n",
              "      <th>X_6</th>\n",
              "      <th>X_7</th>\n",
              "      <th>X_8</th>\n",
              "      <th>X_9</th>\n",
              "      <th>...</th>\n",
              "      <th>X_2863</th>\n",
              "      <th>X_2864</th>\n",
              "      <th>X_2865</th>\n",
              "      <th>X_2866</th>\n",
              "      <th>X_2867</th>\n",
              "      <th>X_2868</th>\n",
              "      <th>X_2869</th>\n",
              "      <th>X_2870</th>\n",
              "      <th>X_2871</th>\n",
              "      <th>LINE_PRODUCT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000</td>\n",
              "      <td>1.92</td>\n",
              "      <td>94.40</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.26</td>\n",
              "      <td>10.02</td>\n",
              "      <td>41.5</td>\n",
              "      <td>...</td>\n",
              "      <td>383.00</td>\n",
              "      <td>368.296296</td>\n",
              "      <td>353.00</td>\n",
              "      <td>39.3400</td>\n",
              "      <td>40.8900</td>\n",
              "      <td>32.5600</td>\n",
              "      <td>34.0900</td>\n",
              "      <td>77.770</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_001</td>\n",
              "      <td>1.92</td>\n",
              "      <td>94.40</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.26</td>\n",
              "      <td>10.02</td>\n",
              "      <td>41.5</td>\n",
              "      <td>...</td>\n",
              "      <td>383.00</td>\n",
              "      <td>367.735849</td>\n",
              "      <td>353.00</td>\n",
              "      <td>38.8900</td>\n",
              "      <td>42.8200</td>\n",
              "      <td>43.9200</td>\n",
              "      <td>35.3400</td>\n",
              "      <td>72.550</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_002</td>\n",
              "      <td>2.58</td>\n",
              "      <td>97.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.00</td>\n",
              "      <td>367.320755</td>\n",
              "      <td>353.00</td>\n",
              "      <td>39.1900</td>\n",
              "      <td>36.6500</td>\n",
              "      <td>42.4700</td>\n",
              "      <td>36.5300</td>\n",
              "      <td>78.350</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_003</td>\n",
              "      <td>2.58</td>\n",
              "      <td>97.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>384.00</td>\n",
              "      <td>369.188679</td>\n",
              "      <td>353.00</td>\n",
              "      <td>37.7400</td>\n",
              "      <td>39.1700</td>\n",
              "      <td>52.1700</td>\n",
              "      <td>30.5800</td>\n",
              "      <td>71.780</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_004</td>\n",
              "      <td>2.58</td>\n",
              "      <td>97.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.00</td>\n",
              "      <td>367.351852</td>\n",
              "      <td>352.00</td>\n",
              "      <td>38.7000</td>\n",
              "      <td>41.8900</td>\n",
              "      <td>46.9300</td>\n",
              "      <td>33.0900</td>\n",
              "      <td>76.970</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>593</th>\n",
              "      <td>TRAIN_593</td>\n",
              "      <td>2.00</td>\n",
              "      <td>95.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>52.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.446</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594</th>\n",
              "      <td>TRAIN_594</td>\n",
              "      <td>1.92</td>\n",
              "      <td>94.40</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.26</td>\n",
              "      <td>10.02</td>\n",
              "      <td>41.5</td>\n",
              "      <td>...</td>\n",
              "      <td>384.00</td>\n",
              "      <td>369.811321</td>\n",
              "      <td>353.00</td>\n",
              "      <td>49.4700</td>\n",
              "      <td>53.0700</td>\n",
              "      <td>50.8900</td>\n",
              "      <td>55.1000</td>\n",
              "      <td>66.490</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>TRAIN_595</td>\n",
              "      <td>1.92</td>\n",
              "      <td>94.40</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.26</td>\n",
              "      <td>10.02</td>\n",
              "      <td>41.5</td>\n",
              "      <td>...</td>\n",
              "      <td>383.00</td>\n",
              "      <td>367.018868</td>\n",
              "      <td>352.00</td>\n",
              "      <td>47.1100</td>\n",
              "      <td>49.5134</td>\n",
              "      <td>49.2178</td>\n",
              "      <td>47.3692</td>\n",
              "      <td>68.439</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>TRAIN_596</td>\n",
              "      <td>40.00</td>\n",
              "      <td>94.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>11.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>31.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.446</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>TRAIN_597</td>\n",
              "      <td>21.00</td>\n",
              "      <td>87.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>61.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>52.0</td>\n",
              "      <td>...</td>\n",
              "      <td>383.82</td>\n",
              "      <td>367.359196</td>\n",
              "      <td>350.78</td>\n",
              "      <td>52.1678</td>\n",
              "      <td>55.9140</td>\n",
              "      <td>49.1138</td>\n",
              "      <td>53.4380</td>\n",
              "      <td>65.446</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>598 rows × 2795 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b690bab-3254-4eb0-bb37-afe1a07534d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b690bab-3254-4eb0-bb37-afe1a07534d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b690bab-3254-4eb0-bb37-afe1a07534d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vif 기반 변수 줄이기"
      ],
      "metadata": {
        "id": "8K1e7zv0Z5ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calc_vif(v):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = v.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(v.values,i) for i in range(v.shape[1])]\n",
        "\n",
        "    high_vif = vif[vif[\"VIF\"] > 5].sort_values(\"VIF\",ascending=False)\n",
        "\n",
        "    \n",
        "    return(high_vif)"
      ],
      "metadata": {
        "id": "07IZglDFZaNx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = train_x.corr()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Find features with correlation greater than 0.95\n",
        "to_drop1 = [column for column in upper.columns if any(upper[column] < -0.9)]\n",
        "to_drop2 = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "print(to_drop1)\n",
        "print(to_drop2)\n",
        "drop=to_drop1+to_drop2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Tr6yV-uZcpj",
        "outputId": "f10f5784-0caa-4d5f-f7f8-96671f06c7d8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['X_20', 'X_21', 'X_22', 'X_38', 'X_44', 'X_47', 'X_48', 'X_50', 'X_53', 'X_63', 'X_65', 'X_86', 'X_87', 'X_88', 'X_97', 'X_98', 'X_99', 'X_101', 'X_103', 'X_117', 'X_119', 'X_120', 'X_121', 'X_123', 'X_148', 'X_149', 'X_150', 'X_242', 'X_243', 'X_265', 'X_266', 'X_267', 'X_295', 'X_303', 'X_306', 'X_320', 'X_321', 'X_331', 'X_332', 'X_333', 'X_364', 'X_365', 'X_366', 'X_367', 'X_368', 'X_370', 'X_480', 'X_512', 'X_551', 'X_568', 'X_685', 'X_781', 'X_854', 'X_855', 'X_910', 'X_912', 'X_960', 'X_961', 'X_962', 'X_963', 'X_964', 'X_965', 'X_978', 'X_979', 'X_981', 'X_982', 'X_983', 'X_1010', 'X_1012', 'X_1018', 'X_1019', 'X_1026', 'X_1027', 'X_1028', 'X_1029', 'X_1030', 'X_1031', 'X_1032', 'X_1034', 'X_1035', 'X_1036', 'X_1037', 'X_1038', 'X_1039', 'X_1042', 'X_1043', 'X_1044', 'X_1045', 'X_1046', 'X_1048', 'X_1049', 'X_1051', 'X_1052', 'X_1053', 'X_1054', 'X_1055', 'X_1056', 'X_1059', 'X_1060', 'X_1061', 'X_1062', 'X_1063', 'X_1065', 'X_1066', 'X_1068', 'X_1069', 'X_1071', 'X_1072', 'X_1073', 'X_1079', 'X_1080', 'X_1081', 'X_1082', 'X_1083', 'X_1084', 'X_1085', 'X_1086', 'X_1087', 'X_1090', 'X_1091', 'X_1093', 'X_1094', 'X_1095', 'X_1096', 'X_1102', 'X_1106', 'X_1107', 'X_1108', 'X_1109', 'X_1110', 'X_1111', 'X_1112', 'X_1113', 'X_1114', 'X_1117', 'X_1118', 'X_1120', 'X_1121', 'X_1122', 'X_1123', 'X_1127', 'X_1129', 'X_1133', 'X_1134', 'X_1135', 'X_1136', 'X_1138', 'X_1139', 'X_1140', 'X_1141', 'X_1144', 'X_1145', 'X_1147', 'X_1148', 'X_1149', 'X_1150', 'X_1154', 'X_1156', 'X_1159', 'X_1160', 'X_1161', 'X_1164', 'X_1165', 'X_1166', 'X_1176', 'X_1180', 'X_1184', 'X_1185', 'X_1186', 'X_1187', 'X_1189', 'X_1193', 'X_1196', 'X_1198', 'X_1202', 'X_1203', 'X_1207', 'X_1208', 'X_1217', 'X_1218', 'X_1221', 'X_1228', 'X_1229', 'X_1236', 'X_1238', 'X_1247', 'X_1262', 'X_1281', 'X_1285', 'X_1288', 'X_1289', 'X_1290', 'X_1292', 'X_1297', 'X_1300', 'X_1302', 'X_1304', 'X_1321', 'X_1325', 'X_1327', 'X_1329', 'X_1330', 'X_1331', 'X_1332', 'X_1333', 'X_1334', 'X_1335', 'X_1338', 'X_1339', 'X_1343', 'X_1344', 'X_1348', 'X_1349', 'X_1352', 'X_1353', 'X_1354', 'X_1355', 'X_1359', 'X_1360', 'X_1385', 'X_1400', 'X_1402', 'X_1403', 'X_1405', 'X_1410', 'X_1411', 'X_1412', 'X_1413', 'X_1414', 'X_1415', 'X_1417', 'X_1419', 'X_1420', 'X_1421', 'X_1422', 'X_1423', 'X_1424', 'X_1428', 'X_1429', 'X_1433', 'X_1434', 'X_1435', 'X_1436', 'X_1437', 'X_1438', 'X_1439', 'X_1440', 'X_1441', 'X_1444', 'X_1445', 'X_1446', 'X_1449', 'X_1450', 'X_1451', 'X_1452', 'X_1453', 'X_1454', 'X_1455', 'X_1456', 'X_1458', 'X_1459', 'X_1460', 'X_1461', 'X_1462', 'X_1463', 'X_1464', 'X_1465', 'X_1466', 'X_1476', 'X_1477', 'X_1478', 'X_1479', 'X_1480', 'X_1481', 'X_1482', 'X_1483', 'X_1484', 'X_1485', 'X_1486', 'X_1488', 'X_1489', 'X_1490', 'X_1491', 'X_1492', 'X_1493', 'X_1500', 'X_1501', 'X_1505', 'X_1506', 'X_1507', 'X_1508', 'X_1509', 'X_1510', 'X_1511', 'X_1512', 'X_1513', 'X_1514', 'X_1515', 'X_1516', 'X_1517', 'X_1520', 'X_1529', 'X_1534', 'X_1536', 'X_1542', 'X_1543', 'X_1547', 'X_1548', 'X_1549', 'X_1550', 'X_1551', 'X_1552', 'X_1556', 'X_1562', 'X_1563', 'X_1568', 'X_1569', 'X_1570', 'X_1583', 'X_1584', 'X_1585', 'X_1586', 'X_1587', 'X_1588', 'X_1591', 'X_1592', 'X_1593', 'X_1594', 'X_1595', 'X_1596', 'X_1597', 'X_1598', 'X_1599', 'X_1600', 'X_1601', 'X_1602', 'X_1605', 'X_1606', 'X_1609', 'X_1610', 'X_1611', 'X_1612', 'X_1613', 'X_1614', 'X_1615', 'X_1616', 'X_1617', 'X_1618', 'X_1624', 'X_1625', 'X_1626', 'X_1629', 'X_1630', 'X_1631', 'X_1632', 'X_1633', 'X_1634', 'X_1635', 'X_1636', 'X_1637', 'X_1638', 'X_1639', 'X_1640', 'X_1641', 'X_1642', 'X_1643', 'X_1648', 'X_1649', 'X_1650', 'X_1651', 'X_1652', 'X_1655', 'X_1656', 'X_1657', 'X_1658', 'X_1659', 'X_1660', 'X_1661', 'X_1662', 'X_1663', 'X_1664', 'X_1665', 'X_1671', 'X_1672', 'X_1673', 'X_1674', 'X_1678', 'X_1679', 'X_1686', 'X_1706', 'X_1717', 'X_1719', 'X_1725', 'X_1726', 'X_1730', 'X_1732', 'X_1734', 'X_1735', 'X_1737', 'X_1739', 'X_1740', 'X_1758', 'X_1762', 'X_1764', 'X_1770', 'X_1794', 'X_1804', 'X_1805', 'X_1806', 'X_1809', 'X_1810', 'X_1812', 'X_1814', 'X_1824', 'X_1825', 'X_1826', 'X_1829', 'X_1830', 'X_1832', 'X_1834', 'X_1849', 'X_1850', 'X_1873', 'X_1879', 'X_1881', 'X_1883', 'X_1885', 'X_1887', 'X_1889', 'X_1891', 'X_1893', 'X_1897', 'X_1899', 'X_1901', 'X_1903', 'X_1905', 'X_1907', 'X_1909', 'X_1911', 'X_1913', 'X_1915', 'X_1917', 'X_1919', 'X_1923', 'X_1925', 'X_1927', 'X_1929', 'X_1935', 'X_1937', 'X_1943', 'X_1948', 'X_1971', 'X_1973', 'X_1974', 'X_1975', 'X_1976', 'X_1977', 'X_1978', 'X_1979', 'X_1980', 'X_1981', 'X_1982', 'X_1983', 'X_1986', 'X_1987', 'X_1988', 'X_1989', 'X_1990', 'X_1992', 'X_1993', 'X_1994', 'X_1995', 'X_1997', 'X_1998', 'X_1999', 'X_2000', 'X_2001', 'X_2002', 'X_2003', 'X_2004', 'X_2005', 'X_2006', 'X_2007', 'X_2009', 'X_2010', 'X_2011', 'X_2012', 'X_2013', 'X_2014', 'X_2015', 'X_2016', 'X_2018', 'X_2019', 'X_2024', 'X_2025', 'X_2029', 'X_2030', 'X_2031', 'X_2034', 'X_2035', 'X_2036', 'X_2037', 'X_2038', 'X_2040', 'X_2044', 'X_2045', 'X_2046', 'X_2047', 'X_2095', 'X_2187', 'X_2189', 'X_2191', 'X_2192', 'X_2193', 'X_2198', 'X_2199', 'X_2200', 'X_2201', 'X_2202', 'X_2203', 'X_2204', 'X_2205', 'X_2208', 'X_2209', 'X_2210', 'X_2211', 'X_2212', 'X_2213', 'X_2216', 'X_2217', 'X_2218', 'X_2219', 'X_2220', 'X_2221', 'X_2222', 'X_2223', 'X_2224', 'X_2225', 'X_2226', 'X_2227', 'X_2228', 'X_2229', 'X_2230', 'X_2231', 'X_2232', 'X_2233', 'X_2234', 'X_2235', 'X_2236', 'X_2237', 'X_2238', 'X_2239', 'X_2242', 'X_2243', 'X_2244', 'X_2245', 'X_2248', 'X_2249', 'X_2254', 'X_2255', 'X_2256', 'X_2257', 'X_2262', 'X_2263', 'X_2264', 'X_2265', 'X_2270', 'X_2271', 'X_2272', 'X_2273', 'X_2274', 'X_2275', 'X_2276', 'X_2277', 'X_2280', 'X_2281', 'X_2282', 'X_2283', 'X_2284', 'X_2285', 'X_2288', 'X_2289', 'X_2290', 'X_2291', 'X_2292', 'X_2293', 'X_2294', 'X_2295', 'X_2296', 'X_2297', 'X_2298', 'X_2299', 'X_2300', 'X_2301', 'X_2302', 'X_2303', 'X_2304', 'X_2305', 'X_2306', 'X_2307', 'X_2308', 'X_2309', 'X_2310', 'X_2311', 'X_2314', 'X_2315', 'X_2316', 'X_2317', 'X_2320', 'X_2321', 'X_2326', 'X_2327', 'X_2328', 'X_2329', 'X_2334', 'X_2335', 'X_2336', 'X_2337', 'X_2342', 'X_2343', 'X_2344', 'X_2345', 'X_2346', 'X_2347', 'X_2348', 'X_2349', 'X_2352', 'X_2353', 'X_2354', 'X_2355', 'X_2356', 'X_2357', 'X_2360', 'X_2361', 'X_2362', 'X_2363', 'X_2364', 'X_2365', 'X_2366', 'X_2367', 'X_2368', 'X_2369', 'X_2370', 'X_2371', 'X_2372', 'X_2373', 'X_2374', 'X_2375', 'X_2376', 'X_2377', 'X_2378', 'X_2379', 'X_2380', 'X_2381', 'X_2382', 'X_2383', 'X_2386', 'X_2387', 'X_2388', 'X_2389', 'X_2392', 'X_2393', 'X_2398', 'X_2399', 'X_2400', 'X_2401', 'X_2406', 'X_2407', 'X_2414', 'X_2415', 'X_2417', 'X_2421', 'X_2422', 'X_2423', 'X_2428', 'X_2429', 'X_2430', 'X_2431', 'X_2432', 'X_2467', 'X_2468', 'X_2469', 'X_2474', 'X_2475', 'X_2476', 'X_2477', 'X_2478', 'X_2479', 'X_2480', 'X_2481', 'X_2482', 'X_2483', 'X_2484', 'X_2485', 'X_2486', 'X_2487', 'X_2488', 'X_2489', 'X_2492', 'X_2493', 'X_2494', 'X_2495', 'X_2496', 'X_2497', 'X_2498', 'X_2499', 'X_2500', 'X_2501', 'X_2502', 'X_2503', 'X_2504', 'X_2505', 'X_2506', 'X_2507', 'X_2508', 'X_2509', 'X_2510', 'X_2511', 'X_2512', 'X_2513', 'X_2514', 'X_2515', 'X_2518', 'X_2519', 'X_2520', 'X_2521', 'X_2522', 'X_2523', 'X_2524', 'X_2525', 'X_2530', 'X_2531', 'X_2532', 'X_2533', 'X_2538', 'X_2539', 'X_2547', 'X_2550', 'X_2552', 'X_2553', 'X_2554', 'X_2555', 'X_2556', 'X_2557', 'X_2562', 'X_2563', 'X_2564', 'X_2565', 'X_2566', 'X_2567', 'X_2568', 'X_2569', 'X_2572', 'X_2573', 'X_2574', 'X_2575', 'X_2576', 'X_2577', 'X_2580', 'X_2581', 'X_2582', 'X_2583', 'X_2584', 'X_2585', 'X_2586', 'X_2587', 'X_2588', 'X_2589', 'X_2590', 'X_2591', 'X_2592', 'X_2593', 'X_2594', 'X_2595', 'X_2596', 'X_2597', 'X_2598', 'X_2599', 'X_2600', 'X_2601', 'X_2602', 'X_2603', 'X_2606', 'X_2607', 'X_2608', 'X_2609', 'X_2612', 'X_2613', 'X_2618', 'X_2619', 'X_2620', 'X_2621', 'X_2626', 'X_2627', 'X_2704', 'X_2705', 'X_2706', 'X_2707', 'X_2708', 'X_2726', 'X_2727', 'X_2728', 'X_2730', 'X_2734', 'X_2735', 'X_2736', 'X_2741', 'X_2742', 'X_2743', 'X_2744', 'X_2745', 'X_2780', 'X_2783', 'X_2786', 'X_2802', 'X_2805', 'X_2806', 'X_2807', 'X_2808', 'X_2809', 'X_2810', 'X_2811', 'X_2812', 'X_2814', 'X_2815', 'X_2816', 'X_2817', 'X_2818', 'X_2819', 'X_2820', 'X_2821', 'X_2822', 'X_2823', 'X_2824', 'X_2826', 'X_2827', 'X_2828', 'X_2829', 'X_2832', 'X_2833', 'X_2836', 'X_2845', 'X_2846', 'X_2847', 'X_2858', 'X_2859', 'X_2863', 'X_2864', 'X_2865']\n",
            "['X_12', 'X_13', 'X_21', 'X_22', 'X_25', 'X_40', 'X_41', 'X_42', 'X_44', 'X_47', 'X_48', 'X_49', 'X_50', 'X_51', 'X_52', 'X_53', 'X_54', 'X_55', 'X_56', 'X_57', 'X_58', 'X_59', 'X_60', 'X_61', 'X_62', 'X_63', 'X_64', 'X_65', 'X_66', 'X_86', 'X_87', 'X_88', 'X_97', 'X_98', 'X_99', 'X_101', 'X_103', 'X_117', 'X_119', 'X_121', 'X_123', 'X_125', 'X_127', 'X_129', 'X_131', 'X_137', 'X_138', 'X_140', 'X_149', 'X_150', 'X_197', 'X_199', 'X_200', 'X_201', 'X_202', 'X_203', 'X_204', 'X_205', 'X_206', 'X_207', 'X_208', 'X_209', 'X_210', 'X_211', 'X_212', 'X_213', 'X_214', 'X_215', 'X_216', 'X_219', 'X_220', 'X_221', 'X_222', 'X_223', 'X_224', 'X_225', 'X_226', 'X_227', 'X_228', 'X_229', 'X_230', 'X_231', 'X_238', 'X_243', 'X_245', 'X_254', 'X_257', 'X_258', 'X_266', 'X_267', 'X_270', 'X_273', 'X_277', 'X_281', 'X_287', 'X_288', 'X_291', 'X_293', 'X_294', 'X_296', 'X_297', 'X_299', 'X_300', 'X_301', 'X_302', 'X_303', 'X_305', 'X_309', 'X_320', 'X_321', 'X_331', 'X_332', 'X_333', 'X_342', 'X_358', 'X_364', 'X_365', 'X_366', 'X_368', 'X_370', 'X_372', 'X_374', 'X_385', 'X_386', 'X_401', 'X_402', 'X_416', 'X_417', 'X_418', 'X_419', 'X_420', 'X_421', 'X_422', 'X_423', 'X_428', 'X_445', 'X_449', 'X_455', 'X_459', 'X_465', 'X_469', 'X_472', 'X_488', 'X_490', 'X_491', 'X_494', 'X_498', 'X_504', 'X_506', 'X_507', 'X_510', 'X_512', 'X_514', 'X_521', 'X_523', 'X_525', 'X_526', 'X_527', 'X_528', 'X_534', 'X_535', 'X_540', 'X_541', 'X_547', 'X_549', 'X_551', 'X_575', 'X_578', 'X_579', 'X_614', 'X_621', 'X_625', 'X_626', 'X_627', 'X_628', 'X_629', 'X_630', 'X_632', 'X_635', 'X_637', 'X_643', 'X_644', 'X_645', 'X_646', 'X_647', 'X_648', 'X_649', 'X_651', 'X_652', 'X_653', 'X_654', 'X_655', 'X_656', 'X_657', 'X_663', 'X_665', 'X_696', 'X_697', 'X_701', 'X_702', 'X_703', 'X_704', 'X_705', 'X_706', 'X_707', 'X_708', 'X_712', 'X_713', 'X_719', 'X_720', 'X_721', 'X_722', 'X_723', 'X_724', 'X_725', 'X_726', 'X_727', 'X_728', 'X_730', 'X_731', 'X_733', 'X_734', 'X_735', 'X_736', 'X_737', 'X_738', 'X_739', 'X_740', 'X_741', 'X_742', 'X_744', 'X_745', 'X_746', 'X_747', 'X_748', 'X_750', 'X_751', 'X_752', 'X_753', 'X_762', 'X_771', 'X_772', 'X_774', 'X_775', 'X_797', 'X_798', 'X_799', 'X_800', 'X_801', 'X_802', 'X_803', 'X_804', 'X_805', 'X_806', 'X_807', 'X_808', 'X_809', 'X_810', 'X_811', 'X_812', 'X_814', 'X_817', 'X_818', 'X_819', 'X_820', 'X_821', 'X_831', 'X_832', 'X_833', 'X_834', 'X_841', 'X_842', 'X_847', 'X_852', 'X_853', 'X_855', 'X_856', 'X_857', 'X_858', 'X_861', 'X_864', 'X_868', 'X_875', 'X_876', 'X_877', 'X_891', 'X_892', 'X_893', 'X_895', 'X_896', 'X_910', 'X_911', 'X_912', 'X_913', 'X_914', 'X_915', 'X_916', 'X_917', 'X_918', 'X_919', 'X_920', 'X_921', 'X_922', 'X_923', 'X_924', 'X_925', 'X_926', 'X_927', 'X_928', 'X_929', 'X_930', 'X_939', 'X_940', 'X_942', 'X_943', 'X_944', 'X_945', 'X_946', 'X_958', 'X_959', 'X_961', 'X_962', 'X_963', 'X_964', 'X_965', 'X_967', 'X_978', 'X_979', 'X_981', 'X_982', 'X_983', 'X_985', 'X_986', 'X_994', 'X_995', 'X_996', 'X_997', 'X_998', 'X_1000', 'X_1001', 'X_1010', 'X_1011', 'X_1012', 'X_1018', 'X_1019', 'X_1026', 'X_1027', 'X_1028', 'X_1029', 'X_1030', 'X_1031', 'X_1032', 'X_1034', 'X_1035', 'X_1036', 'X_1037', 'X_1038', 'X_1039', 'X_1040', 'X_1042', 'X_1043', 'X_1044', 'X_1045', 'X_1046', 'X_1047', 'X_1048', 'X_1049', 'X_1050', 'X_1051', 'X_1052', 'X_1053', 'X_1054', 'X_1055', 'X_1056', 'X_1057', 'X_1059', 'X_1060', 'X_1061', 'X_1062', 'X_1063', 'X_1064', 'X_1065', 'X_1066', 'X_1067', 'X_1068', 'X_1069', 'X_1071', 'X_1072', 'X_1073', 'X_1074', 'X_1079', 'X_1080', 'X_1081', 'X_1082', 'X_1083', 'X_1084', 'X_1085', 'X_1086', 'X_1087', 'X_1088', 'X_1089', 'X_1090', 'X_1091', 'X_1093', 'X_1094', 'X_1095', 'X_1096', 'X_1102', 'X_1104', 'X_1105', 'X_1106', 'X_1107', 'X_1108', 'X_1109', 'X_1110', 'X_1111', 'X_1112', 'X_1113', 'X_1114', 'X_1115', 'X_1116', 'X_1117', 'X_1118', 'X_1120', 'X_1121', 'X_1122', 'X_1123', 'X_1125', 'X_1126', 'X_1127', 'X_1128', 'X_1129', 'X_1131', 'X_1132', 'X_1133', 'X_1134', 'X_1135', 'X_1136', 'X_1138', 'X_1139', 'X_1140', 'X_1141', 'X_1142', 'X_1143', 'X_1144', 'X_1145', 'X_1147', 'X_1148', 'X_1149', 'X_1150', 'X_1151', 'X_1152', 'X_1153', 'X_1154', 'X_1155', 'X_1156', 'X_1159', 'X_1160', 'X_1161', 'X_1163', 'X_1164', 'X_1165', 'X_1166', 'X_1172', 'X_1173', 'X_1176', 'X_1177', 'X_1180', 'X_1181', 'X_1184', 'X_1185', 'X_1186', 'X_1187', 'X_1189', 'X_1190', 'X_1192', 'X_1193', 'X_1196', 'X_1198', 'X_1202', 'X_1203', 'X_1207', 'X_1208', 'X_1217', 'X_1218', 'X_1221', 'X_1224', 'X_1228', 'X_1229', 'X_1236', 'X_1238', 'X_1241', 'X_1245', 'X_1247', 'X_1262', 'X_1281', 'X_1285', 'X_1288', 'X_1289', 'X_1290', 'X_1292', 'X_1297', 'X_1299', 'X_1300', 'X_1302', 'X_1303', 'X_1304', 'X_1305', 'X_1306', 'X_1307', 'X_1308', 'X_1310', 'X_1313', 'X_1315', 'X_1321', 'X_1322', 'X_1323', 'X_1324', 'X_1325', 'X_1326', 'X_1327', 'X_1329', 'X_1330', 'X_1331', 'X_1332', 'X_1333', 'X_1334', 'X_1335', 'X_1338', 'X_1339', 'X_1343', 'X_1344', 'X_1345', 'X_1346', 'X_1348', 'X_1349', 'X_1350', 'X_1351', 'X_1352', 'X_1353', 'X_1354', 'X_1355', 'X_1358', 'X_1359', 'X_1360', 'X_1385', 'X_1400', 'X_1401', 'X_1402', 'X_1403', 'X_1404', 'X_1405', 'X_1410', 'X_1411', 'X_1412', 'X_1413', 'X_1414', 'X_1415', 'X_1416', 'X_1417', 'X_1418', 'X_1419', 'X_1420', 'X_1421', 'X_1422', 'X_1423', 'X_1425', 'X_1428', 'X_1429', 'X_1430', 'X_1431', 'X_1432', 'X_1433', 'X_1434', 'X_1435', 'X_1436', 'X_1437', 'X_1438', 'X_1439', 'X_1440', 'X_1441', 'X_1442', 'X_1443', 'X_1444', 'X_1445', 'X_1446', 'X_1447', 'X_1448', 'X_1449', 'X_1450', 'X_1451', 'X_1452', 'X_1453', 'X_1454', 'X_1455', 'X_1456', 'X_1458', 'X_1459', 'X_1460', 'X_1461', 'X_1462', 'X_1463', 'X_1464', 'X_1465', 'X_1466', 'X_1467', 'X_1468', 'X_1469', 'X_1470', 'X_1471', 'X_1472', 'X_1473', 'X_1474', 'X_1475', 'X_1476', 'X_1477', 'X_1478', 'X_1479', 'X_1480', 'X_1481', 'X_1482', 'X_1483', 'X_1484', 'X_1485', 'X_1486', 'X_1488', 'X_1489', 'X_1490', 'X_1491', 'X_1492', 'X_1493', 'X_1494', 'X_1495', 'X_1496', 'X_1497', 'X_1498', 'X_1499', 'X_1500', 'X_1501', 'X_1505', 'X_1506', 'X_1507', 'X_1508', 'X_1509', 'X_1510', 'X_1511', 'X_1512', 'X_1513', 'X_1514', 'X_1515', 'X_1516', 'X_1517', 'X_1520', 'X_1523', 'X_1524', 'X_1525', 'X_1526', 'X_1527', 'X_1528', 'X_1529', 'X_1534', 'X_1536', 'X_1538', 'X_1539', 'X_1541', 'X_1542', 'X_1543', 'X_1544', 'X_1545', 'X_1546', 'X_1547', 'X_1548', 'X_1549', 'X_1550', 'X_1551', 'X_1552', 'X_1553', 'X_1556', 'X_1557', 'X_1562', 'X_1563', 'X_1565', 'X_1566', 'X_1567', 'X_1568', 'X_1569', 'X_1570', 'X_1583', 'X_1584', 'X_1585', 'X_1586', 'X_1587', 'X_1588', 'X_1589', 'X_1590', 'X_1591', 'X_1592', 'X_1593', 'X_1594', 'X_1595', 'X_1596', 'X_1597', 'X_1598', 'X_1599', 'X_1600', 'X_1601', 'X_1602', 'X_1603', 'X_1604', 'X_1605', 'X_1606', 'X_1607', 'X_1608', 'X_1609', 'X_1610', 'X_1611', 'X_1612', 'X_1613', 'X_1614', 'X_1615', 'X_1616', 'X_1617', 'X_1618', 'X_1619', 'X_1620', 'X_1622', 'X_1623', 'X_1624', 'X_1625', 'X_1626', 'X_1627', 'X_1628', 'X_1629', 'X_1630', 'X_1631', 'X_1632', 'X_1633', 'X_1634', 'X_1635', 'X_1636', 'X_1637', 'X_1638', 'X_1639', 'X_1640', 'X_1641', 'X_1642', 'X_1643', 'X_1648', 'X_1649', 'X_1650', 'X_1651', 'X_1652', 'X_1653', 'X_1654', 'X_1655', 'X_1656', 'X_1657', 'X_1658', 'X_1659', 'X_1660', 'X_1661', 'X_1662', 'X_1663', 'X_1664', 'X_1665', 'X_1667', 'X_1671', 'X_1672', 'X_1673', 'X_1674', 'X_1678', 'X_1679', 'X_1696', 'X_1697', 'X_1710', 'X_1714', 'X_1725', 'X_1726', 'X_1727', 'X_1728', 'X_1730', 'X_1732', 'X_1734', 'X_1735', 'X_1737', 'X_1738', 'X_1739', 'X_1740', 'X_1756', 'X_1758', 'X_1762', 'X_1764', 'X_1770', 'X_1794', 'X_1804', 'X_1805', 'X_1806', 'X_1809', 'X_1810', 'X_1812', 'X_1814', 'X_1824', 'X_1825', 'X_1826', 'X_1829', 'X_1830', 'X_1832', 'X_1833', 'X_1834', 'X_1849', 'X_1850', 'X_1855', 'X_1862', 'X_1873', 'X_1874', 'X_1875', 'X_1876', 'X_1877', 'X_1878', 'X_1879', 'X_1880', 'X_1881', 'X_1883', 'X_1884', 'X_1885', 'X_1886', 'X_1887', 'X_1888', 'X_1889', 'X_1890', 'X_1891', 'X_1892', 'X_1893', 'X_1894', 'X_1895', 'X_1896', 'X_1897', 'X_1898', 'X_1899', 'X_1900', 'X_1901', 'X_1902', 'X_1903', 'X_1904', 'X_1905', 'X_1906', 'X_1907', 'X_1908', 'X_1909', 'X_1910', 'X_1911', 'X_1912', 'X_1913', 'X_1914', 'X_1915', 'X_1916', 'X_1917', 'X_1918', 'X_1919', 'X_1920', 'X_1921', 'X_1922', 'X_1923', 'X_1924', 'X_1925', 'X_1926', 'X_1927', 'X_1928', 'X_1929', 'X_1930', 'X_1931', 'X_1932', 'X_1933', 'X_1934', 'X_1935', 'X_1936', 'X_1937', 'X_1938', 'X_1939', 'X_1940', 'X_1941', 'X_1942', 'X_1943', 'X_1944', 'X_1945', 'X_1948', 'X_1950', 'X_1961', 'X_1963', 'X_1965', 'X_1966', 'X_1971', 'X_1975', 'X_1976', 'X_1977', 'X_1978', 'X_1979', 'X_1980', 'X_1981', 'X_1982', 'X_1983', 'X_1984', 'X_1986', 'X_1987', 'X_1988', 'X_1989', 'X_1990', 'X_1992', 'X_1993', 'X_1994', 'X_1995', 'X_1996', 'X_1997', 'X_1998', 'X_1999', 'X_2000', 'X_2001', 'X_2002', 'X_2003', 'X_2004', 'X_2005', 'X_2006', 'X_2007', 'X_2008', 'X_2009', 'X_2010', 'X_2011', 'X_2012', 'X_2013', 'X_2014', 'X_2015', 'X_2016', 'X_2017', 'X_2018', 'X_2019', 'X_2020', 'X_2021', 'X_2022', 'X_2023', 'X_2024', 'X_2025', 'X_2028', 'X_2029', 'X_2030', 'X_2031', 'X_2032', 'X_2033', 'X_2034', 'X_2035', 'X_2036', 'X_2037', 'X_2038', 'X_2039', 'X_2040', 'X_2042', 'X_2043', 'X_2044', 'X_2045', 'X_2046', 'X_2047', 'X_2049', 'X_2060', 'X_2070', 'X_2071', 'X_2072', 'X_2073', 'X_2074', 'X_2075', 'X_2077', 'X_2079', 'X_2080', 'X_2081', 'X_2082', 'X_2083', 'X_2084', 'X_2085', 'X_2086', 'X_2087', 'X_2088', 'X_2089', 'X_2090', 'X_2091', 'X_2092', 'X_2093', 'X_2095', 'X_2098', 'X_2100', 'X_2101', 'X_2102', 'X_2103', 'X_2104', 'X_2105', 'X_2106', 'X_2107', 'X_2108', 'X_2109', 'X_2110', 'X_2111', 'X_2112', 'X_2113', 'X_2115', 'X_2116', 'X_2117', 'X_2118', 'X_2119', 'X_2120', 'X_2121', 'X_2122', 'X_2123', 'X_2124', 'X_2125', 'X_2126', 'X_2127', 'X_2128', 'X_2129', 'X_2130', 'X_2131', 'X_2132', 'X_2133', 'X_2134', 'X_2135', 'X_2136', 'X_2137', 'X_2138', 'X_2139', 'X_2140', 'X_2141', 'X_2142', 'X_2143', 'X_2144', 'X_2145', 'X_2146', 'X_2147', 'X_2148', 'X_2149', 'X_2150', 'X_2151', 'X_2152', 'X_2153', 'X_2154', 'X_2155', 'X_2156', 'X_2157', 'X_2158', 'X_2159', 'X_2160', 'X_2161', 'X_2162', 'X_2163', 'X_2164', 'X_2165', 'X_2166', 'X_2167', 'X_2168', 'X_2169', 'X_2171', 'X_2172', 'X_2173', 'X_2174', 'X_2175', 'X_2176', 'X_2177', 'X_2178', 'X_2179', 'X_2180', 'X_2181', 'X_2182', 'X_2183', 'X_2189', 'X_2192', 'X_2193', 'X_2194', 'X_2195', 'X_2196', 'X_2197', 'X_2198', 'X_2199', 'X_2200', 'X_2201', 'X_2202', 'X_2203', 'X_2204', 'X_2205', 'X_2206', 'X_2207', 'X_2208', 'X_2209', 'X_2210', 'X_2211', 'X_2212', 'X_2213', 'X_2214', 'X_2215', 'X_2216', 'X_2217', 'X_2218', 'X_2219', 'X_2220', 'X_2221', 'X_2222', 'X_2223', 'X_2224', 'X_2225', 'X_2226', 'X_2227', 'X_2228', 'X_2229', 'X_2230', 'X_2231', 'X_2232', 'X_2233', 'X_2234', 'X_2235', 'X_2236', 'X_2237', 'X_2238', 'X_2239', 'X_2240', 'X_2241', 'X_2242', 'X_2243', 'X_2244', 'X_2245', 'X_2246', 'X_2247', 'X_2248', 'X_2249', 'X_2250', 'X_2251', 'X_2252', 'X_2253', 'X_2254', 'X_2255', 'X_2256', 'X_2257', 'X_2258', 'X_2259', 'X_2260', 'X_2261', 'X_2262', 'X_2263', 'X_2264', 'X_2265', 'X_2266', 'X_2267', 'X_2268', 'X_2269', 'X_2270', 'X_2271', 'X_2272', 'X_2273', 'X_2274', 'X_2275', 'X_2276', 'X_2277', 'X_2278', 'X_2279', 'X_2280', 'X_2281', 'X_2282', 'X_2283', 'X_2284', 'X_2285', 'X_2286', 'X_2287', 'X_2288', 'X_2289', 'X_2290', 'X_2291', 'X_2292', 'X_2293', 'X_2294', 'X_2295', 'X_2296', 'X_2297', 'X_2298', 'X_2299', 'X_2300', 'X_2301', 'X_2302', 'X_2303', 'X_2304', 'X_2305', 'X_2306', 'X_2307', 'X_2308', 'X_2309', 'X_2310', 'X_2311', 'X_2312', 'X_2313', 'X_2314', 'X_2315', 'X_2316', 'X_2317', 'X_2318', 'X_2319', 'X_2320', 'X_2321', 'X_2322', 'X_2323', 'X_2324', 'X_2325', 'X_2326', 'X_2327', 'X_2328', 'X_2329', 'X_2330', 'X_2331', 'X_2332', 'X_2333', 'X_2334', 'X_2335', 'X_2336', 'X_2337', 'X_2338', 'X_2339', 'X_2340', 'X_2341', 'X_2342', 'X_2343', 'X_2344', 'X_2345', 'X_2346', 'X_2347', 'X_2348', 'X_2349', 'X_2350', 'X_2351', 'X_2352', 'X_2353', 'X_2354', 'X_2355', 'X_2356', 'X_2357', 'X_2358', 'X_2359', 'X_2360', 'X_2361', 'X_2362', 'X_2363', 'X_2364', 'X_2365', 'X_2366', 'X_2367', 'X_2368', 'X_2369', 'X_2370', 'X_2371', 'X_2372', 'X_2373', 'X_2374', 'X_2375', 'X_2376', 'X_2377', 'X_2378', 'X_2379', 'X_2380', 'X_2381', 'X_2382', 'X_2383', 'X_2384', 'X_2385', 'X_2386', 'X_2387', 'X_2388', 'X_2389', 'X_2390', 'X_2391', 'X_2392', 'X_2393', 'X_2394', 'X_2395', 'X_2396', 'X_2397', 'X_2398', 'X_2399', 'X_2400', 'X_2401', 'X_2402', 'X_2403', 'X_2404', 'X_2405', 'X_2406', 'X_2407', 'X_2409', 'X_2411', 'X_2414', 'X_2415', 'X_2417', 'X_2418', 'X_2421', 'X_2422', 'X_2423', 'X_2424', 'X_2427', 'X_2428', 'X_2429', 'X_2430', 'X_2431', 'X_2432', 'X_2461', 'X_2465', 'X_2466', 'X_2467', 'X_2468', 'X_2469', 'X_2470', 'X_2471', 'X_2472', 'X_2473', 'X_2474', 'X_2475', 'X_2476', 'X_2477', 'X_2478', 'X_2479', 'X_2480', 'X_2481', 'X_2482', 'X_2483', 'X_2484', 'X_2485', 'X_2486', 'X_2487', 'X_2488', 'X_2489', 'X_2490', 'X_2491', 'X_2492', 'X_2493', 'X_2494', 'X_2495', 'X_2496', 'X_2497', 'X_2498', 'X_2499', 'X_2500', 'X_2501', 'X_2502', 'X_2503', 'X_2504', 'X_2505', 'X_2506', 'X_2507', 'X_2508', 'X_2509', 'X_2510', 'X_2511', 'X_2512', 'X_2513', 'X_2514', 'X_2515', 'X_2516', 'X_2517', 'X_2518', 'X_2519', 'X_2520', 'X_2521', 'X_2522', 'X_2523', 'X_2524', 'X_2525', 'X_2526', 'X_2527', 'X_2528', 'X_2529', 'X_2530', 'X_2531', 'X_2532', 'X_2533', 'X_2534', 'X_2535', 'X_2536', 'X_2537', 'X_2538', 'X_2539', 'X_2543', 'X_2544', 'X_2545', 'X_2546', 'X_2547', 'X_2548', 'X_2550', 'X_2552', 'X_2553', 'X_2554', 'X_2555', 'X_2556', 'X_2557', 'X_2558', 'X_2559', 'X_2560', 'X_2561', 'X_2562', 'X_2563', 'X_2564', 'X_2565', 'X_2566', 'X_2567', 'X_2568', 'X_2569', 'X_2570', 'X_2571', 'X_2572', 'X_2573', 'X_2574', 'X_2575', 'X_2576', 'X_2577', 'X_2578', 'X_2579', 'X_2580', 'X_2581', 'X_2582', 'X_2583', 'X_2584', 'X_2585', 'X_2586', 'X_2587', 'X_2588', 'X_2589', 'X_2590', 'X_2591', 'X_2592', 'X_2593', 'X_2594', 'X_2595', 'X_2596', 'X_2597', 'X_2598', 'X_2599', 'X_2600', 'X_2601', 'X_2602', 'X_2603', 'X_2604', 'X_2605', 'X_2606', 'X_2607', 'X_2608', 'X_2609', 'X_2610', 'X_2611', 'X_2612', 'X_2613', 'X_2614', 'X_2615', 'X_2616', 'X_2617', 'X_2618', 'X_2619', 'X_2620', 'X_2621', 'X_2622', 'X_2623', 'X_2624', 'X_2625', 'X_2626', 'X_2627', 'X_2700', 'X_2701', 'X_2702', 'X_2704', 'X_2705', 'X_2706', 'X_2707', 'X_2708', 'X_2709', 'X_2710', 'X_2711', 'X_2712', 'X_2713', 'X_2714', 'X_2715', 'X_2716', 'X_2717', 'X_2718', 'X_2719', 'X_2720', 'X_2722', 'X_2724', 'X_2725', 'X_2726', 'X_2727', 'X_2728', 'X_2730', 'X_2734', 'X_2735', 'X_2736', 'X_2737', 'X_2741', 'X_2742', 'X_2743', 'X_2744', 'X_2745', 'X_2753', 'X_2774', 'X_2778', 'X_2779', 'X_2780', 'X_2781', 'X_2782', 'X_2783', 'X_2786', 'X_2788', 'X_2791', 'X_2802', 'X_2803', 'X_2804', 'X_2805', 'X_2806', 'X_2807', 'X_2808', 'X_2809', 'X_2810', 'X_2811', 'X_2812', 'X_2813', 'X_2814', 'X_2815', 'X_2816', 'X_2817', 'X_2818', 'X_2819', 'X_2820', 'X_2821', 'X_2822', 'X_2823', 'X_2824', 'X_2825', 'X_2826', 'X_2827', 'X_2828', 'X_2829', 'X_2830', 'X_2831', 'X_2832', 'X_2833', 'X_2834', 'X_2835', 'X_2836', 'X_2841', 'X_2843', 'X_2845', 'X_2846', 'X_2847', 'X_2848', 'X_2849', 'X_2850', 'X_2851', 'X_2852', 'X_2853', 'X_2854', 'X_2855', 'X_2856', 'X_2857', 'X_2858', 'X_2859', 'X_2860', 'X_2861', 'X_2862', 'X_2863', 'X_2864', 'X_2865', 'X_2866', 'X_2867', 'X_2868', 'X_2869', 'X_2870']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(drop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnlGuHZZZmzP",
        "outputId": "527af946-f7dd-4edf-f001-11a729588c92"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2419"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_x.drop(drop,1)\n",
        "test_x = test_x.drop(drop,1)"
      ],
      "metadata": {
        "id": "0SdkVeQxZr53"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkI569FQZ2Bf",
        "outputId": "28d04524-a175-4f10-8c84-a8271496d45a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(598, 1201)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_train = train_x.copy()\n",
        "features_test = test_x.copy()"
      ],
      "metadata": {
        "id": "trzxS7gxij-O"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.iloc[:,1:].columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfAZ1bV-i5Ha",
        "outputId": "d8a6f45f-c3e8-46f9-98f8-d58509943b79"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['X_1', 'X_2', 'X_3', 'X_4', 'X_5', 'X_6', 'X_7', 'X_8', 'X_9', 'X_10',\n",
              "       ...\n",
              "       'X_2798', 'X_2799', 'X_2800', 'X_2801', 'X_2837', 'X_2839', 'X_2840',\n",
              "       'X_2842', 'X_2871', 'LINE_PRODUCT'],\n",
              "      dtype='object', length=1200)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8VuSB2Fn7QL",
        "outputId": "377d8f32-b5ad-4729-d464-6c16daa0fdc5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(598, 1201)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqfQ5RCrn9ME",
        "outputId": "d0dac8b5-37b9-4a00-8a78-0904ea42db46"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(310, 1201)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MinMaxScaler"
      ],
      "metadata": {
        "id": "SIDhZx2R2d6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "2h045EFbhWok"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()"
      ],
      "metadata": {
        "id": "2PQ5h_pG3cJQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train data 변환\n",
        "X_group_train = scaler.fit_transform(features_train.iloc[:,1:])\n",
        "\n",
        "#test data 변환\n",
        "X_group_test = scaler.transform(features_test.iloc[:,1:])"
      ],
      "metadata": {
        "id": "B7KVcPA6AGn8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.iloc[:,1:] = pd.DataFrame(X_group_train, columns=train_x.iloc[:,1:].columns, index=list(train_x.iloc[:,1:].index.values));train_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "wlshHUMn5AH6",
        "outputId": "6823f191-2a9c-4b67-cf67-432778742400"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    PRODUCT_ID       X_1       X_2  X_3  X_4   X_5  X_6       X_7   X_8  X_9  \\\n",
              "0    TRAIN_000  0.009020  0.493333  0.0  0.0  0.44  0.0  0.191765  0.02  0.5   \n",
              "1    TRAIN_001  0.009020  0.493333  0.0  0.0  0.44  0.0  0.191765  0.02  0.5   \n",
              "2    TRAIN_002  0.015490  0.725333  0.0  0.0  0.60  0.0  0.000000  0.00  0.0   \n",
              "3    TRAIN_003  0.015490  0.725333  0.0  0.0  0.60  0.0  0.000000  0.00  0.0   \n",
              "4    TRAIN_004  0.015490  0.725333  0.0  0.0  0.60  0.0  0.000000  0.00  0.0   \n",
              "..         ...       ...       ...  ...  ...   ...  ...       ...   ...  ...   \n",
              "593  TRAIN_593  0.009804  0.533333  0.0  0.0  0.00  0.0  0.294118  0.00  1.0   \n",
              "594  TRAIN_594  0.009020  0.493333  0.0  0.0  0.44  0.0  0.191765  0.02  0.5   \n",
              "595  TRAIN_595  0.009020  0.493333  0.0  0.0  0.44  0.0  0.191765  0.02  0.5   \n",
              "596  TRAIN_596  0.382353  0.466667  0.0  0.0  1.00  0.0  0.000000  0.00  0.0   \n",
              "597  TRAIN_597  0.196078  0.000000  0.0  0.0  0.00  0.0  0.941176  0.00  1.0   \n",
              "\n",
              "     ...  X_2798  X_2799    X_2800  X_2801    X_2837    X_2839    X_2840  \\\n",
              "0    ...    0.00   0.250  0.211356    0.00  0.577143  0.514563  0.910714   \n",
              "1    ...    0.50   0.500  0.748993    1.00  0.280000  0.495146  1.000000   \n",
              "2    ...    0.50   0.500  0.473847    0.50  0.577143  0.514563  0.053571   \n",
              "3    ...    0.50   0.500  0.745712    0.75  0.134286  0.504854  0.964286   \n",
              "4    ...    0.50   0.500  0.483221    0.50  0.577143  0.524272  0.053571   \n",
              "..   ...     ...     ...       ...     ...       ...       ...       ...   \n",
              "593  ...    0.54   0.615  0.621319    0.58  0.339429  0.455728  0.824286   \n",
              "594  ...    0.50   0.750  0.600740    0.50  0.425714  0.485437  0.910714   \n",
              "595  ...    0.50   0.750  0.582660    0.50  0.562857  0.495146  0.910714   \n",
              "596  ...    0.54   0.615  0.621319    0.58  0.339429  0.455728  0.824286   \n",
              "597  ...    0.54   0.615  0.621319    0.58  0.339429  0.455728  0.824286   \n",
              "\n",
              "       X_2842  X_2871  LINE_PRODUCT  \n",
              "0    0.048394     0.0      0.285714  \n",
              "1    0.236239     0.0      0.428571  \n",
              "2    0.048624     0.0      0.285714  \n",
              "3    0.250000     0.0      0.428571  \n",
              "4    0.049083     0.0      0.285714  \n",
              "..        ...     ...           ...  \n",
              "593  0.141440     0.0      1.000000  \n",
              "594  0.000917     0.0      0.285714  \n",
              "595  0.017202     0.0      0.285714  \n",
              "596  0.141440     0.0      0.571429  \n",
              "597  0.141440     0.0      0.857143  \n",
              "\n",
              "[598 rows x 1201 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-604c6408-7a81-4443-bd6e-c7933bcb167d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>X_1</th>\n",
              "      <th>X_2</th>\n",
              "      <th>X_3</th>\n",
              "      <th>X_4</th>\n",
              "      <th>X_5</th>\n",
              "      <th>X_6</th>\n",
              "      <th>X_7</th>\n",
              "      <th>X_8</th>\n",
              "      <th>X_9</th>\n",
              "      <th>...</th>\n",
              "      <th>X_2798</th>\n",
              "      <th>X_2799</th>\n",
              "      <th>X_2800</th>\n",
              "      <th>X_2801</th>\n",
              "      <th>X_2837</th>\n",
              "      <th>X_2839</th>\n",
              "      <th>X_2840</th>\n",
              "      <th>X_2842</th>\n",
              "      <th>X_2871</th>\n",
              "      <th>LINE_PRODUCT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000</td>\n",
              "      <td>0.009020</td>\n",
              "      <td>0.493333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.191765</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.211356</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.577143</td>\n",
              "      <td>0.514563</td>\n",
              "      <td>0.910714</td>\n",
              "      <td>0.048394</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_001</td>\n",
              "      <td>0.009020</td>\n",
              "      <td>0.493333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.191765</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.748993</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.495146</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.236239</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.428571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_002</td>\n",
              "      <td>0.015490</td>\n",
              "      <td>0.725333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.473847</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.577143</td>\n",
              "      <td>0.514563</td>\n",
              "      <td>0.053571</td>\n",
              "      <td>0.048624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_003</td>\n",
              "      <td>0.015490</td>\n",
              "      <td>0.725333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.745712</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.134286</td>\n",
              "      <td>0.504854</td>\n",
              "      <td>0.964286</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.428571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_004</td>\n",
              "      <td>0.015490</td>\n",
              "      <td>0.725333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.483221</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.577143</td>\n",
              "      <td>0.524272</td>\n",
              "      <td>0.053571</td>\n",
              "      <td>0.049083</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>593</th>\n",
              "      <td>TRAIN_593</td>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.615</td>\n",
              "      <td>0.621319</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.339429</td>\n",
              "      <td>0.455728</td>\n",
              "      <td>0.824286</td>\n",
              "      <td>0.141440</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594</th>\n",
              "      <td>TRAIN_594</td>\n",
              "      <td>0.009020</td>\n",
              "      <td>0.493333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.191765</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.600740</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.425714</td>\n",
              "      <td>0.485437</td>\n",
              "      <td>0.910714</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>TRAIN_595</td>\n",
              "      <td>0.009020</td>\n",
              "      <td>0.493333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.191765</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.582660</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.562857</td>\n",
              "      <td>0.495146</td>\n",
              "      <td>0.910714</td>\n",
              "      <td>0.017202</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>TRAIN_596</td>\n",
              "      <td>0.382353</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.615</td>\n",
              "      <td>0.621319</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.339429</td>\n",
              "      <td>0.455728</td>\n",
              "      <td>0.824286</td>\n",
              "      <td>0.141440</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>TRAIN_597</td>\n",
              "      <td>0.196078</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.615</td>\n",
              "      <td>0.621319</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.339429</td>\n",
              "      <td>0.455728</td>\n",
              "      <td>0.824286</td>\n",
              "      <td>0.141440</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>598 rows × 1201 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-604c6408-7a81-4443-bd6e-c7933bcb167d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-604c6408-7a81-4443-bd6e-c7933bcb167d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-604c6408-7a81-4443-bd6e-c7933bcb167d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_x.iloc[:,1:] = pd.DataFrame(X_group_test, columns=test_x.iloc[:,1:].columns, index=list(test_x.iloc[:,1:].index.values))"
      ],
      "metadata": {
        "id": "YvV3Yj4L4J-s"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 분류기"
      ],
      "metadata": {
        "id": "5s25vdvcdRv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "7J-Ma4cpdcVj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [['Naive Bayes :', GaussianNB()],\n",
        "               ['KNeighbours :', KNeighborsClassifier()],\n",
        "               ['SVM :', SVC()],\n",
        "               ['LogisticRegression :', LogisticRegression()],\n",
        "               ['DecisionTree :',DecisionTreeClassifier()],\n",
        "               ['RandomForest :',RandomForestClassifier()],\n",
        "               ['LGBMClassifier:', LGBMClassifier()],\n",
        "               ['XGBClassifier: ', XGBClassifier()]]"
      ],
      "metadata": {
        "id": "v3jVwB8RdTcw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "osmote=SMOTE()\n",
        "Xs_train,ys_train=osmote.fit_resample(train_x.iloc[:,1:],train_y)\n",
        "\n",
        "print(Counter(ys_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0m5vTVMdm6b",
        "outputId": "8811b7a7-e509-4ba0-ba7d-33b24c88e588"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({1: 407, 2: 407, 0: 407})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "jqv_T-NwhD1o"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xs_train, Xs_valid, ys_train, ys_valid = train_test_split(Xs_train, ys_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "NHq8LJuyhI6x"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name,classifier in classifiers:\n",
        "    clf=classifier.fit(Xs_train,ys_train)\n",
        "    y_pred=classifier.predict(Xs_valid)\n",
        "    print(f'\\n {name} \\n')\n",
        "    print(f'Training Score for {name}  {clf.score(Xs_train,ys_train) * 100:.2f}' )\n",
        "    print(f'Testing Score for {name} {clf.score(Xs_valid,ys_valid) * 100:.2f}' )\n",
        "    print(f'Classification report  \\n {classification_report(ys_valid,y_pred)}' )\n",
        "    print(f'Confusion matrix  \\n {confusion_matrix(ys_valid,y_pred)}' )\n",
        "    #print(f'ROC AUC  : {roc_auc_score(valid_y,y_pred)}' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D63ZUf_MkPy0",
        "outputId": "8b483c50-080b-4854-a42b-be56f7a023f0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Naive Bayes : \n",
            "\n",
            "Training Score for Naive Bayes :  63.93\n",
            "Testing Score for Naive Bayes : 62.04\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.49      0.64        83\n",
            "           1       0.51      0.73      0.60        82\n",
            "           2       0.61      0.64      0.63        80\n",
            "\n",
            "    accuracy                           0.62       245\n",
            "   macro avg       0.68      0.62      0.62       245\n",
            "weighted avg       0.68      0.62      0.62       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[41 29 13]\n",
            " [ 3 60 19]\n",
            " [ 1 28 51]]\n",
            "\n",
            " KNeighbours : \n",
            "\n",
            "Training Score for KNeighbours :  75.51\n",
            "Testing Score for KNeighbours : 68.57\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.99      0.80        83\n",
            "           1       0.89      0.10      0.18        82\n",
            "           2       0.68      0.97      0.80        80\n",
            "\n",
            "    accuracy                           0.69       245\n",
            "   macro avg       0.75      0.69      0.59       245\n",
            "weighted avg       0.75      0.69      0.59       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[82  1  0]\n",
            " [37  8 37]\n",
            " [ 2  0 78]]\n",
            "\n",
            " SVM : \n",
            "\n",
            "Training Score for SVM :  82.07\n",
            "Testing Score for SVM : 79.18\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.87      0.88        83\n",
            "           1       0.67      0.80      0.73        82\n",
            "           2       0.86      0.70      0.77        80\n",
            "\n",
            "    accuracy                           0.79       245\n",
            "   macro avg       0.81      0.79      0.79       245\n",
            "weighted avg       0.81      0.79      0.79       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[72  9  2]\n",
            " [ 9 66  7]\n",
            " [ 0 24 56]]\n",
            "\n",
            " LogisticRegression : \n",
            "\n",
            "Training Score for LogisticRegression :  99.69\n",
            "Testing Score for LogisticRegression : 86.53\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.94        83\n",
            "           1       0.83      0.76      0.79        82\n",
            "           2       0.85      0.88      0.86        80\n",
            "\n",
            "    accuracy                           0.87       245\n",
            "   macro avg       0.86      0.86      0.86       245\n",
            "weighted avg       0.86      0.87      0.86       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[80  3  0]\n",
            " [ 8 62 12]\n",
            " [ 0 10 70]]\n",
            "\n",
            " DecisionTree : \n",
            "\n",
            "Training Score for DecisionTree :  100.00\n",
            "Testing Score for DecisionTree : 82.45\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86        83\n",
            "           1       0.79      0.76      0.77        82\n",
            "           2       0.83      0.84      0.83        80\n",
            "\n",
            "    accuracy                           0.82       245\n",
            "   macro avg       0.82      0.82      0.82       245\n",
            "weighted avg       0.82      0.82      0.82       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[73  6  4]\n",
            " [10 62 10]\n",
            " [ 3 10 67]]\n",
            "\n",
            " RandomForest : \n",
            "\n",
            "Training Score for RandomForest :  100.00\n",
            "Testing Score for RandomForest : 88.57\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.93        83\n",
            "           1       0.81      0.88      0.84        82\n",
            "           2       0.94      0.82      0.88        80\n",
            "\n",
            "    accuracy                           0.89       245\n",
            "   macro avg       0.89      0.88      0.89       245\n",
            "weighted avg       0.89      0.89      0.89       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[79  4  0]\n",
            " [ 6 72  4]\n",
            " [ 1 13 66]]\n",
            "\n",
            " LGBMClassifier: \n",
            "\n",
            "Training Score for LGBMClassifier:  100.00\n",
            "Testing Score for LGBMClassifier: 89.39\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.94      0.94        83\n",
            "           1       0.81      0.90      0.86        82\n",
            "           2       0.94      0.84      0.89        80\n",
            "\n",
            "    accuracy                           0.89       245\n",
            "   macro avg       0.90      0.89      0.89       245\n",
            "weighted avg       0.90      0.89      0.89       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[78  5  0]\n",
            " [ 4 74  4]\n",
            " [ 1 12 67]]\n",
            "\n",
            " XGBClassifier:  \n",
            "\n",
            "Training Score for XGBClassifier:   100.00\n",
            "Testing Score for XGBClassifier:  87.35\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90        83\n",
            "           1       0.78      0.88      0.83        82\n",
            "           2       0.96      0.84      0.89        80\n",
            "\n",
            "    accuracy                           0.87       245\n",
            "   macro avg       0.88      0.87      0.87       245\n",
            "weighted avg       0.88      0.87      0.87       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[75  8  0]\n",
            " [ 7 72  3]\n",
            " [ 1 12 67]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.combine import SMOTETomek\n",
        "rus=SMOTETomek()\n",
        "Xrus_train,yrus_train =rus.fit_resample (train_x.iloc[:,1:],train_y)\n",
        "\n",
        "print(Counter(yrus_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbLxUuB0mGRd",
        "outputId": "9e2bf14f-9634-4885-afaf-5e80de6c1e77"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({1: 407, 2: 407, 0: 407})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xrus_train, Xrus_valid, yrus_train, yrus_valid = train_test_split(Xrus_train, yrus_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Jxweo8fAmrZn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name,classifier in classifiers:\n",
        "    clf=classifier.fit(Xrus_train,yrus_train)\n",
        "    y_pred=classifier.predict(Xrus_valid)\n",
        "    print(f'\\n {name} \\n')\n",
        "    print(f'Training Score for {name}  {clf.score(Xrus_train,yrus_train) * 100:.2f}' )\n",
        "    print(f'Testing Score for {name} {clf.score(Xrus_valid,yrus_valid) * 100:.2f}' )\n",
        "    print(f'Classification report  \\n {classification_report(yrus_valid,y_pred)}' )\n",
        "    print(f'Confusion matrix  \\n {confusion_matrix(yrus_valid,y_pred)}' )\n",
        "    #print(f'ROC AUC  : {roc_auc_score(valid_y,y_pred)}' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orcxUU_Smyin",
        "outputId": "7a3ac7af-c660-43c6-dc1e-54d05386485b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Naive Bayes : \n",
            "\n",
            "Training Score for Naive Bayes :  63.52\n",
            "Testing Score for Naive Bayes : 60.82\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.47      0.61        83\n",
            "           1       0.51      0.73      0.60        82\n",
            "           2       0.61      0.62      0.62        80\n",
            "\n",
            "    accuracy                           0.61       245\n",
            "   macro avg       0.66      0.61      0.61       245\n",
            "weighted avg       0.66      0.61      0.61       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[39 30 14]\n",
            " [ 4 60 18]\n",
            " [ 2 28 50]]\n",
            "\n",
            " KNeighbours : \n",
            "\n",
            "Training Score for KNeighbours :  72.85\n",
            "Testing Score for KNeighbours : 66.53\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      1.00      0.79        83\n",
            "           1       1.00      0.05      0.09        82\n",
            "           2       0.67      0.95      0.79        80\n",
            "\n",
            "    accuracy                           0.67       245\n",
            "   macro avg       0.77      0.67      0.56       245\n",
            "weighted avg       0.77      0.67      0.55       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[83  0  0]\n",
            " [41  4 37]\n",
            " [ 4  0 76]]\n",
            "\n",
            " SVM : \n",
            "\n",
            "Training Score for SVM :  83.40\n",
            "Testing Score for SVM : 79.18\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86        83\n",
            "           1       0.70      0.76      0.73        82\n",
            "           2       0.83      0.75      0.79        80\n",
            "\n",
            "    accuracy                           0.79       245\n",
            "   macro avg       0.79      0.79      0.79       245\n",
            "weighted avg       0.79      0.79      0.79       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[72  6  5]\n",
            " [13 62  7]\n",
            " [ 0 20 60]]\n",
            "\n",
            " LogisticRegression : \n",
            "\n",
            "Training Score for LogisticRegression :  99.69\n",
            "Testing Score for LogisticRegression : 90.20\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      1.00      0.95        83\n",
            "           1       0.95      0.74      0.84        82\n",
            "           2       0.87      0.96      0.91        80\n",
            "\n",
            "    accuracy                           0.90       245\n",
            "   macro avg       0.91      0.90      0.90       245\n",
            "weighted avg       0.91      0.90      0.90       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[83  0  0]\n",
            " [ 9 61 12]\n",
            " [ 0  3 77]]\n",
            "\n",
            " DecisionTree : \n",
            "\n",
            "Training Score for DecisionTree :  100.00\n",
            "Testing Score for DecisionTree : 77.96\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.81      0.79        83\n",
            "           1       0.71      0.74      0.73        82\n",
            "           2       0.88      0.79      0.83        80\n",
            "\n",
            "    accuracy                           0.78       245\n",
            "   macro avg       0.78      0.78      0.78       245\n",
            "weighted avg       0.78      0.78      0.78       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[67 13  3]\n",
            " [15 61  6]\n",
            " [ 5 12 63]]\n",
            "\n",
            " RandomForest : \n",
            "\n",
            "Training Score for RandomForest :  100.00\n",
            "Testing Score for RandomForest : 91.43\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94        83\n",
            "           1       0.87      0.89      0.88        82\n",
            "           2       0.96      0.89      0.92        80\n",
            "\n",
            "    accuracy                           0.91       245\n",
            "   macro avg       0.92      0.91      0.91       245\n",
            "weighted avg       0.92      0.91      0.91       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[80  2  1]\n",
            " [ 7 73  2]\n",
            " [ 0  9 71]]\n",
            "\n",
            " LGBMClassifier: \n",
            "\n",
            "Training Score for LGBMClassifier:  100.00\n",
            "Testing Score for LGBMClassifier: 91.84\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.94        83\n",
            "           1       0.88      0.88      0.88        82\n",
            "           2       0.97      0.91      0.94        80\n",
            "\n",
            "    accuracy                           0.92       245\n",
            "   macro avg       0.92      0.92      0.92       245\n",
            "weighted avg       0.92      0.92      0.92       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[80  3  0]\n",
            " [ 8 72  2]\n",
            " [ 0  7 73]]\n",
            "\n",
            " XGBClassifier:  \n",
            "\n",
            "Training Score for XGBClassifier:   99.59\n",
            "Testing Score for XGBClassifier:  89.80\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.94      0.92        83\n",
            "           1       0.86      0.84      0.85        82\n",
            "           2       0.94      0.91      0.92        80\n",
            "\n",
            "    accuracy                           0.90       245\n",
            "   macro avg       0.90      0.90      0.90       245\n",
            "weighted avg       0.90      0.90      0.90       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[78  4  1]\n",
            " [ 9 69  4]\n",
            " [ 0  7 73]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "\n",
        "cc=BorderlineSMOTE()\n",
        "Xcc_train,ycc_train =cc.fit_resample (train_x.iloc[:,1:],train_y)\n",
        "\n",
        "print(Counter(ycc_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXSdyJ6knHux",
        "outputId": "0e9140d5-86cf-40c4-a0ae-94bf93fa1fe8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({1: 407, 2: 407, 0: 407})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xcc_train, Xcc_valid, ycc_train, ycc_valid = train_test_split(Xcc_train, ycc_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5OoQ9oycnb3U"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name,classifier in classifiers:\n",
        "    clf=classifier.fit(Xcc_train,ycc_train)\n",
        "    y_pred=classifier.predict(Xcc_valid)\n",
        "    print(f'\\n {name} \\n')\n",
        "    print(f'Training Score for {name}  {clf.score(Xcc_train,ycc_train) * 100:.2f}' )\n",
        "    print(f'Testing Score for {name} {clf.score(Xcc_valid,ycc_valid) * 100:.2f}' )\n",
        "    print(f'Classification report  \\n {classification_report(ycc_valid,y_pred)}' )\n",
        "    print(f'Confusion matrix  \\n {confusion_matrix(ycc_valid,y_pred)}' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXpgF9s8nmY7",
        "outputId": "74297a38-217c-45fe-c897-92738346b452"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Naive Bayes : \n",
            "\n",
            "Training Score for Naive Bayes :  64.34\n",
            "Testing Score for Naive Bayes : 57.55\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.46      0.59        83\n",
            "           1       0.48      0.73      0.58        82\n",
            "           2       0.58      0.54      0.56        80\n",
            "\n",
            "    accuracy                           0.58       245\n",
            "   macro avg       0.63      0.58      0.58       245\n",
            "weighted avg       0.64      0.58      0.58       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[38 30 15]\n",
            " [ 6 60 16]\n",
            " [ 1 36 43]]\n",
            "\n",
            " KNeighbours : \n",
            "\n",
            "Training Score for KNeighbours :  75.31\n",
            "Testing Score for KNeighbours : 69.39\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.99      0.80        83\n",
            "           1       0.86      0.15      0.25        82\n",
            "           2       0.69      0.95      0.80        80\n",
            "\n",
            "    accuracy                           0.69       245\n",
            "   macro avg       0.74      0.69      0.62       245\n",
            "weighted avg       0.74      0.69      0.62       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[82  1  0]\n",
            " [36 12 34]\n",
            " [ 3  1 76]]\n",
            "\n",
            " SVM : \n",
            "\n",
            "Training Score for SVM :  86.07\n",
            "Testing Score for SVM : 80.00\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.83      0.85        83\n",
            "           1       0.69      0.78      0.73        82\n",
            "           2       0.86      0.79      0.82        80\n",
            "\n",
            "    accuracy                           0.80       245\n",
            "   macro avg       0.81      0.80      0.80       245\n",
            "weighted avg       0.81      0.80      0.80       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[69 13  1]\n",
            " [ 9 64  9]\n",
            " [ 1 16 63]]\n",
            "\n",
            " LogisticRegression : \n",
            "\n",
            "Training Score for LogisticRegression :  99.69\n",
            "Testing Score for LogisticRegression : 88.98\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92        83\n",
            "           1       0.92      0.73      0.82        82\n",
            "           2       0.87      0.97      0.92        80\n",
            "\n",
            "    accuracy                           0.89       245\n",
            "   macro avg       0.89      0.89      0.89       245\n",
            "weighted avg       0.89      0.89      0.89       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[80  3  0]\n",
            " [10 60 12]\n",
            " [ 0  2 78]]\n",
            "\n",
            " DecisionTree : \n",
            "\n",
            "Training Score for DecisionTree :  100.00\n",
            "Testing Score for DecisionTree : 82.04\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.87      0.88        83\n",
            "           1       0.80      0.72      0.76        82\n",
            "           2       0.77      0.88      0.82        80\n",
            "\n",
            "    accuracy                           0.82       245\n",
            "   macro avg       0.82      0.82      0.82       245\n",
            "weighted avg       0.82      0.82      0.82       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[72  7  4]\n",
            " [ 6 59 17]\n",
            " [ 2  8 70]]\n",
            "\n",
            " RandomForest : \n",
            "\n",
            "Training Score for RandomForest :  100.00\n",
            "Testing Score for RandomForest : 90.61\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.92      0.92        83\n",
            "           1       0.84      0.89      0.86        82\n",
            "           2       0.96      0.91      0.94        80\n",
            "\n",
            "    accuracy                           0.91       245\n",
            "   macro avg       0.91      0.91      0.91       245\n",
            "weighted avg       0.91      0.91      0.91       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[76  7  0]\n",
            " [ 6 73  3]\n",
            " [ 0  7 73]]\n",
            "\n",
            " LGBMClassifier: \n",
            "\n",
            "Training Score for LGBMClassifier:  100.00\n",
            "Testing Score for LGBMClassifier: 89.80\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.93      0.91        83\n",
            "           1       0.83      0.88      0.85        82\n",
            "           2       0.99      0.89      0.93        80\n",
            "\n",
            "    accuracy                           0.90       245\n",
            "   macro avg       0.90      0.90      0.90       245\n",
            "weighted avg       0.90      0.90      0.90       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[77  6  0]\n",
            " [ 9 72  1]\n",
            " [ 0  9 71]]\n",
            "\n",
            " XGBClassifier:  \n",
            "\n",
            "Training Score for XGBClassifier:   99.59\n",
            "Testing Score for XGBClassifier:  89.80\n",
            "Classification report  \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.94      0.92        83\n",
            "           1       0.84      0.85      0.85        82\n",
            "           2       0.96      0.90      0.93        80\n",
            "\n",
            "    accuracy                           0.90       245\n",
            "   macro avg       0.90      0.90      0.90       245\n",
            "weighted avg       0.90      0.90      0.90       245\n",
            "\n",
            "Confusion matrix  \n",
            " [[78  5  0]\n",
            " [ 9 70  3]\n",
            " [ 0  8 72]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sMOTETOMEK + votingclassifier"
      ],
      "metadata": {
        "id": "fGRdex-JonSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.combine import SMOTETomek\n",
        "rus=SMOTETomek()\n",
        "Xrus_train,yrus_train =rus.fit_resample (train_x.iloc[:,1:],train_y)\n",
        "\n",
        "print(Counter(yrus_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFLMEEmpoqlm",
        "outputId": "3fdd7a0d-c8d6-4098-d9f9-119f90bf1e05"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({1: 407, 2: 407, 0: 407})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 베이지안 최적화"
      ],
      "metadata": {
        "id": "QoDXonoF1yFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization\n",
        "\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVEaYLL1oyVm",
        "outputId": "bce00921-22e2-4439-c30f-5cbb8c9e489a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.8/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from bayesian-optimization) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from bayesian-optimization) (1.0.2)\n",
            "Requirement already satisfied: colorama>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from bayesian-optimization) (0.4.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from bayesian-optimization) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "rf_params={\n",
        "    'max_depth':(1,150),\n",
        "    'n_estimators':(10,100)\n",
        "}\n",
        "\n",
        "def rf_bo(max_depth, n_estimators):\n",
        "  params={\n",
        "      'max_depth':int(round(max_depth)),\n",
        "      'n_estimators':int(round(n_estimators))\n",
        "  }\n",
        "\n",
        "  model=RandomForestClassifier( **params,  n_jobs=-1, random_state=42)\n",
        "  \n",
        "  X_train,X_valid,y_train,y_valid=train_test_split(Xcc_train.iloc[:,1:],ycc_train,test_size=0.8)\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  score=f1_score(y_valid, model.predict(X_valid), average = 'micro')\n",
        "  return score"
      ],
      "metadata": {
        "id": "lv8avNZao5ej"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BO_rf = BayesianOptimization(f=rf_bo,pbounds=rf_params,random_state=3,verbose=2)\n",
        "\n",
        "BO_rf.maximize(init_points=5, n_iter=100)\n",
        "\n",
        "rf_max_params=BO_rf.max['params']\n",
        "rf_max_params['max_depth']=int(rf_max_params['max_depth'])\n",
        "rf_max_params['n_estimators']=int(rf_max_params['n_estimators'])\n",
        "rf_max_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D4FgFi1pL9e",
        "outputId": "47311e60-d588-4cbc-e1f8-3e0891ea5b52"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | max_depth | n_esti... |\n",
            "-------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.7196   \u001b[0m | \u001b[0m83.07    \u001b[0m | \u001b[0m73.73    \u001b[0m |\n",
            "| \u001b[95m2        \u001b[0m | \u001b[95m0.7772   \u001b[0m | \u001b[95m44.34    \u001b[0m | \u001b[95m55.97    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m0.7106   \u001b[0m | \u001b[0m134.0    \u001b[0m | \u001b[0m90.67    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.7554   \u001b[0m | \u001b[0m19.71    \u001b[0m | \u001b[0m28.65    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.6927   \u001b[0m | \u001b[0m8.669    \u001b[0m | \u001b[0m49.67    \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.7196   \u001b[0m | \u001b[0m45.3     \u001b[0m | \u001b[0m55.69    \u001b[0m |\n",
            "| \u001b[95m7        \u001b[0m | \u001b[95m0.7862   \u001b[0m | \u001b[95m19.66    \u001b[0m | \u001b[95m28.57    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m0.7708   \u001b[0m | \u001b[0m71.25    \u001b[0m | \u001b[0m22.37    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.7106   \u001b[0m | \u001b[0m19.41    \u001b[0m | \u001b[0m28.67    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m0.7734   \u001b[0m | \u001b[0m42.57    \u001b[0m | \u001b[0m93.02    \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m0.7247   \u001b[0m | \u001b[0m63.56    \u001b[0m | \u001b[0m16.24    \u001b[0m |\n",
            "| \u001b[95m12       \u001b[0m | \u001b[95m0.7875   \u001b[0m | \u001b[95m42.2     \u001b[0m | \u001b[95m93.15    \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m0.781    \u001b[0m | \u001b[0m141.3    \u001b[0m | \u001b[0m94.7     \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m0.7452   \u001b[0m | \u001b[0m44.59    \u001b[0m | \u001b[0m56.15    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m0.7388   \u001b[0m | \u001b[0m141.6    \u001b[0m | \u001b[0m94.49    \u001b[0m |\n",
            "| \u001b[0m16       \u001b[0m | \u001b[0m0.7657   \u001b[0m | \u001b[0m42.56    \u001b[0m | \u001b[0m93.07    \u001b[0m |\n",
            "| \u001b[0m17       \u001b[0m | \u001b[0m0.7657   \u001b[0m | \u001b[0m42.42    \u001b[0m | \u001b[0m93.03    \u001b[0m |\n",
            "| \u001b[95m18       \u001b[0m | \u001b[95m0.8092   \u001b[0m | \u001b[95m97.29    \u001b[0m | \u001b[95m72.56    \u001b[0m |\n",
            "| \u001b[0m19       \u001b[0m | \u001b[0m0.7196   \u001b[0m | \u001b[0m41.9     \u001b[0m | \u001b[0m93.14    \u001b[0m |\n",
            "| \u001b[0m20       \u001b[0m | \u001b[0m0.7849   \u001b[0m | \u001b[0m120.6    \u001b[0m | \u001b[0m91.2     \u001b[0m |\n",
            "| \u001b[0m21       \u001b[0m | \u001b[0m0.6991   \u001b[0m | \u001b[0m62.29    \u001b[0m | \u001b[0m68.17    \u001b[0m |\n",
            "| \u001b[0m22       \u001b[0m | \u001b[0m0.7746   \u001b[0m | \u001b[0m135.4    \u001b[0m | \u001b[0m66.14    \u001b[0m |\n",
            "| \u001b[0m23       \u001b[0m | \u001b[0m0.7324   \u001b[0m | \u001b[0m49.5     \u001b[0m | \u001b[0m84.24    \u001b[0m |\n",
            "| \u001b[0m24       \u001b[0m | \u001b[0m0.735    \u001b[0m | \u001b[0m42.26    \u001b[0m | \u001b[0m92.83    \u001b[0m |\n",
            "| \u001b[0m25       \u001b[0m | \u001b[0m0.717    \u001b[0m | \u001b[0m120.7    \u001b[0m | \u001b[0m91.12    \u001b[0m |\n",
            "| \u001b[0m26       \u001b[0m | \u001b[0m0.7478   \u001b[0m | \u001b[0m38.26    \u001b[0m | \u001b[0m43.36    \u001b[0m |\n",
            "| \u001b[0m27       \u001b[0m | \u001b[0m0.7388   \u001b[0m | \u001b[0m92.55    \u001b[0m | \u001b[0m14.71    \u001b[0m |\n",
            "| \u001b[0m28       \u001b[0m | \u001b[0m0.7849   \u001b[0m | \u001b[0m71.16    \u001b[0m | \u001b[0m22.21    \u001b[0m |\n",
            "| \u001b[0m29       \u001b[0m | \u001b[0m0.7414   \u001b[0m | \u001b[0m134.3    \u001b[0m | \u001b[0m24.57    \u001b[0m |\n",
            "| \u001b[0m30       \u001b[0m | \u001b[0m0.6351   \u001b[0m | \u001b[0m3.368    \u001b[0m | \u001b[0m99.12    \u001b[0m |\n",
            "| \u001b[0m31       \u001b[0m | \u001b[0m0.6671   \u001b[0m | \u001b[0m6.231    \u001b[0m | \u001b[0m38.0     \u001b[0m |\n",
            "| \u001b[95m32       \u001b[0m | \u001b[95m0.8156   \u001b[0m | \u001b[95m120.4    \u001b[0m | \u001b[95m91.29    \u001b[0m |\n",
            "| \u001b[0m33       \u001b[0m | \u001b[0m0.7836   \u001b[0m | \u001b[0m135.5    \u001b[0m | \u001b[0m66.17    \u001b[0m |\n",
            "| \u001b[0m34       \u001b[0m | \u001b[0m0.7209   \u001b[0m | \u001b[0m138.3    \u001b[0m | \u001b[0m92.01    \u001b[0m |\n",
            "| \u001b[0m35       \u001b[0m | \u001b[0m0.7286   \u001b[0m | \u001b[0m106.2    \u001b[0m | \u001b[0m94.39    \u001b[0m |\n",
            "| \u001b[0m36       \u001b[0m | \u001b[0m0.7721   \u001b[0m | \u001b[0m15.02    \u001b[0m | \u001b[0m50.66    \u001b[0m |\n",
            "| \u001b[0m37       \u001b[0m | \u001b[0m0.7606   \u001b[0m | \u001b[0m120.0    \u001b[0m | \u001b[0m91.51    \u001b[0m |\n",
            "| \u001b[0m38       \u001b[0m | \u001b[0m0.6837   \u001b[0m | \u001b[0m26.24    \u001b[0m | \u001b[0m11.24    \u001b[0m |\n",
            "| \u001b[0m39       \u001b[0m | \u001b[0m0.7401   \u001b[0m | \u001b[0m42.77    \u001b[0m | \u001b[0m92.86    \u001b[0m |\n",
            "| \u001b[0m40       \u001b[0m | \u001b[0m0.7503   \u001b[0m | \u001b[0m21.62    \u001b[0m | \u001b[0m67.41    \u001b[0m |\n",
            "| \u001b[0m41       \u001b[0m | \u001b[0m0.7964   \u001b[0m | \u001b[0m104.8    \u001b[0m | \u001b[0m56.31    \u001b[0m |\n",
            "| \u001b[0m42       \u001b[0m | \u001b[0m0.7823   \u001b[0m | \u001b[0m104.0    \u001b[0m | \u001b[0m95.54    \u001b[0m |\n",
            "| \u001b[0m43       \u001b[0m | \u001b[0m0.7849   \u001b[0m | \u001b[0m83.03    \u001b[0m | \u001b[0m50.57    \u001b[0m |\n",
            "| \u001b[0m44       \u001b[0m | \u001b[0m0.758    \u001b[0m | \u001b[0m120.3    \u001b[0m | \u001b[0m91.28    \u001b[0m |\n",
            "| \u001b[0m45       \u001b[0m | \u001b[0m0.7516   \u001b[0m | \u001b[0m104.7    \u001b[0m | \u001b[0m56.07    \u001b[0m |\n",
            "| \u001b[0m46       \u001b[0m | \u001b[0m0.7157   \u001b[0m | \u001b[0m97.41    \u001b[0m | \u001b[0m72.57    \u001b[0m |\n",
            "| \u001b[0m47       \u001b[0m | \u001b[0m0.7734   \u001b[0m | \u001b[0m102.3    \u001b[0m | \u001b[0m78.81    \u001b[0m |\n",
            "| \u001b[0m48       \u001b[0m | \u001b[0m0.7375   \u001b[0m | \u001b[0m128.7    \u001b[0m | \u001b[0m93.4     \u001b[0m |\n",
            "| \u001b[0m49       \u001b[0m | \u001b[0m0.7926   \u001b[0m | \u001b[0m116.5    \u001b[0m | \u001b[0m45.34    \u001b[0m |\n",
            "| \u001b[0m50       \u001b[0m | \u001b[0m0.7426   \u001b[0m | \u001b[0m82.99    \u001b[0m | \u001b[0m50.63    \u001b[0m |\n",
            "| \u001b[0m51       \u001b[0m | \u001b[0m0.7362   \u001b[0m | \u001b[0m44.45    \u001b[0m | \u001b[0m55.76    \u001b[0m |\n",
            "| \u001b[0m52       \u001b[0m | \u001b[0m0.7324   \u001b[0m | \u001b[0m68.09    \u001b[0m | \u001b[0m41.51    \u001b[0m |\n",
            "| \u001b[0m53       \u001b[0m | \u001b[0m0.7631   \u001b[0m | \u001b[0m141.1    \u001b[0m | \u001b[0m94.64    \u001b[0m |\n",
            "| \u001b[0m54       \u001b[0m | \u001b[0m0.7798   \u001b[0m | \u001b[0m141.3    \u001b[0m | \u001b[0m94.8     \u001b[0m |\n",
            "| \u001b[0m55       \u001b[0m | \u001b[0m0.7785   \u001b[0m | \u001b[0m136.5    \u001b[0m | \u001b[0m70.66    \u001b[0m |\n",
            "| \u001b[0m56       \u001b[0m | \u001b[0m0.7618   \u001b[0m | \u001b[0m136.6    \u001b[0m | \u001b[0m70.81    \u001b[0m |\n",
            "| \u001b[0m57       \u001b[0m | \u001b[0m0.7862   \u001b[0m | \u001b[0m136.8    \u001b[0m | \u001b[0m70.72    \u001b[0m |\n",
            "| \u001b[0m58       \u001b[0m | \u001b[0m0.7337   \u001b[0m | \u001b[0m83.27    \u001b[0m | \u001b[0m50.41    \u001b[0m |\n",
            "| \u001b[0m59       \u001b[0m | \u001b[0m0.7875   \u001b[0m | \u001b[0m52.82    \u001b[0m | \u001b[0m86.32    \u001b[0m |\n",
            "| \u001b[0m60       \u001b[0m | \u001b[0m0.7465   \u001b[0m | \u001b[0m82.94    \u001b[0m | \u001b[0m50.41    \u001b[0m |\n",
            "| \u001b[0m61       \u001b[0m | \u001b[0m0.7132   \u001b[0m | \u001b[0m19.66    \u001b[0m | \u001b[0m28.55    \u001b[0m |\n",
            "| \u001b[0m62       \u001b[0m | \u001b[0m0.7286   \u001b[0m | \u001b[0m43.88    \u001b[0m | \u001b[0m41.79    \u001b[0m |\n",
            "| \u001b[0m63       \u001b[0m | \u001b[0m0.7401   \u001b[0m | \u001b[0m79.06    \u001b[0m | \u001b[0m22.5     \u001b[0m |\n",
            "| \u001b[0m64       \u001b[0m | \u001b[0m0.7362   \u001b[0m | \u001b[0m82.77    \u001b[0m | \u001b[0m60.16    \u001b[0m |\n",
            "| \u001b[0m65       \u001b[0m | \u001b[0m0.749    \u001b[0m | \u001b[0m34.7     \u001b[0m | \u001b[0m49.92    \u001b[0m |\n",
            "| \u001b[0m66       \u001b[0m | \u001b[0m0.6991   \u001b[0m | \u001b[0m69.14    \u001b[0m | \u001b[0m78.32    \u001b[0m |\n",
            "| \u001b[0m67       \u001b[0m | \u001b[0m0.7529   \u001b[0m | \u001b[0m70.17    \u001b[0m | \u001b[0m80.25    \u001b[0m |\n",
            "| \u001b[0m68       \u001b[0m | \u001b[0m0.7708   \u001b[0m | \u001b[0m85.24    \u001b[0m | \u001b[0m55.87    \u001b[0m |\n",
            "| \u001b[0m69       \u001b[0m | \u001b[0m0.7401   \u001b[0m | \u001b[0m40.1     \u001b[0m | \u001b[0m87.28    \u001b[0m |\n",
            "| \u001b[0m70       \u001b[0m | \u001b[0m0.7529   \u001b[0m | \u001b[0m88.54    \u001b[0m | \u001b[0m93.39    \u001b[0m |\n",
            "| \u001b[0m71       \u001b[0m | \u001b[0m0.7516   \u001b[0m | \u001b[0m11.33    \u001b[0m | \u001b[0m69.86    \u001b[0m |\n",
            "| \u001b[0m72       \u001b[0m | \u001b[0m0.717    \u001b[0m | \u001b[0m95.92    \u001b[0m | \u001b[0m20.7     \u001b[0m |\n",
            "| \u001b[0m73       \u001b[0m | \u001b[0m0.7145   \u001b[0m | \u001b[0m132.8    \u001b[0m | \u001b[0m99.82    \u001b[0m |\n",
            "| \u001b[0m74       \u001b[0m | \u001b[0m0.7439   \u001b[0m | \u001b[0m42.43    \u001b[0m | \u001b[0m23.48    \u001b[0m |\n",
            "| \u001b[95m75       \u001b[0m | \u001b[95m0.8284   \u001b[0m | \u001b[95m71.92    \u001b[0m | \u001b[95m64.55    \u001b[0m |\n",
            "| \u001b[0m76       \u001b[0m | \u001b[0m0.7093   \u001b[0m | \u001b[0m99.08    \u001b[0m | \u001b[0m14.65    \u001b[0m |\n",
            "| \u001b[0m77       \u001b[0m | \u001b[0m0.7785   \u001b[0m | \u001b[0m40.77    \u001b[0m | \u001b[0m53.54    \u001b[0m |\n",
            "| \u001b[0m78       \u001b[0m | \u001b[0m0.767    \u001b[0m | \u001b[0m137.4    \u001b[0m | \u001b[0m56.0     \u001b[0m |\n",
            "| \u001b[0m79       \u001b[0m | \u001b[0m0.7708   \u001b[0m | \u001b[0m97.89    \u001b[0m | \u001b[0m79.31    \u001b[0m |\n",
            "| \u001b[0m80       \u001b[0m | \u001b[0m0.758    \u001b[0m | \u001b[0m83.79    \u001b[0m | \u001b[0m27.41    \u001b[0m |\n",
            "| \u001b[0m81       \u001b[0m | \u001b[0m0.6863   \u001b[0m | \u001b[0m79.23    \u001b[0m | \u001b[0m10.7     \u001b[0m |\n",
            "| \u001b[0m82       \u001b[0m | \u001b[0m0.7478   \u001b[0m | \u001b[0m90.33    \u001b[0m | \u001b[0m61.35    \u001b[0m |\n",
            "| \u001b[0m83       \u001b[0m | \u001b[0m0.7939   \u001b[0m | \u001b[0m46.45    \u001b[0m | \u001b[0m67.11    \u001b[0m |\n",
            "| \u001b[0m84       \u001b[0m | \u001b[0m0.7606   \u001b[0m | \u001b[0m24.05    \u001b[0m | \u001b[0m54.8     \u001b[0m |\n",
            "| \u001b[0m85       \u001b[0m | \u001b[0m0.7964   \u001b[0m | \u001b[0m41.7     \u001b[0m | \u001b[0m58.07    \u001b[0m |\n",
            "| \u001b[0m86       \u001b[0m | \u001b[0m0.767    \u001b[0m | \u001b[0m8.645    \u001b[0m | \u001b[0m31.78    \u001b[0m |\n",
            "| \u001b[0m87       \u001b[0m | \u001b[0m0.7503   \u001b[0m | \u001b[0m90.85    \u001b[0m | \u001b[0m83.92    \u001b[0m |\n",
            "| \u001b[0m88       \u001b[0m | \u001b[0m0.7542   \u001b[0m | \u001b[0m13.34    \u001b[0m | \u001b[0m34.78    \u001b[0m |\n",
            "| \u001b[0m89       \u001b[0m | \u001b[0m0.7567   \u001b[0m | \u001b[0m21.08    \u001b[0m | \u001b[0m58.68    \u001b[0m |\n",
            "| \u001b[0m90       \u001b[0m | \u001b[0m0.7298   \u001b[0m | \u001b[0m45.07    \u001b[0m | \u001b[0m99.97    \u001b[0m |\n",
            "| \u001b[0m91       \u001b[0m | \u001b[0m0.822    \u001b[0m | \u001b[0m137.2    \u001b[0m | \u001b[0m56.99    \u001b[0m |\n",
            "| \u001b[0m92       \u001b[0m | \u001b[0m0.726    \u001b[0m | \u001b[0m127.1    \u001b[0m | \u001b[0m83.79    \u001b[0m |\n",
            "| \u001b[0m93       \u001b[0m | \u001b[0m0.6325   \u001b[0m | \u001b[0m2.574    \u001b[0m | \u001b[0m63.35    \u001b[0m |\n",
            "| \u001b[0m94       \u001b[0m | \u001b[0m0.7695   \u001b[0m | \u001b[0m29.47    \u001b[0m | \u001b[0m24.31    \u001b[0m |\n",
            "| \u001b[0m95       \u001b[0m | \u001b[0m0.7375   \u001b[0m | \u001b[0m79.75    \u001b[0m | \u001b[0m23.94    \u001b[0m |\n",
            "| \u001b[0m96       \u001b[0m | \u001b[0m0.7196   \u001b[0m | \u001b[0m96.35    \u001b[0m | \u001b[0m98.75    \u001b[0m |\n",
            "| \u001b[0m97       \u001b[0m | \u001b[0m0.6556   \u001b[0m | \u001b[0m73.78    \u001b[0m | \u001b[0m12.16    \u001b[0m |\n",
            "| \u001b[0m98       \u001b[0m | \u001b[0m0.7823   \u001b[0m | \u001b[0m140.7    \u001b[0m | \u001b[0m69.69    \u001b[0m |\n",
            "| \u001b[0m99       \u001b[0m | \u001b[0m0.7542   \u001b[0m | \u001b[0m43.55    \u001b[0m | \u001b[0m14.8     \u001b[0m |\n",
            "| \u001b[0m100      \u001b[0m | \u001b[0m0.7567   \u001b[0m | \u001b[0m8.713    \u001b[0m | \u001b[0m54.74    \u001b[0m |\n",
            "| \u001b[0m101      \u001b[0m | \u001b[0m0.7516   \u001b[0m | \u001b[0m37.31    \u001b[0m | \u001b[0m34.16    \u001b[0m |\n",
            "| \u001b[0m102      \u001b[0m | \u001b[0m0.7362   \u001b[0m | \u001b[0m35.6     \u001b[0m | \u001b[0m38.63    \u001b[0m |\n",
            "| \u001b[0m103      \u001b[0m | \u001b[0m0.7746   \u001b[0m | \u001b[0m17.01    \u001b[0m | \u001b[0m77.38    \u001b[0m |\n",
            "| \u001b[0m104      \u001b[0m | \u001b[0m0.7721   \u001b[0m | \u001b[0m65.25    \u001b[0m | \u001b[0m97.46    \u001b[0m |\n",
            "| \u001b[0m105      \u001b[0m | \u001b[0m0.7823   \u001b[0m | \u001b[0m73.79    \u001b[0m | \u001b[0m95.45    \u001b[0m |\n",
            "=================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 71, 'n_estimators': 64}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_params={\n",
        "    'gamma':(0,20),\n",
        "    'max_depth':(1,2000),\n",
        "    'subsample':(0.5,1),\n",
        "    'eta' : (0.001, 0.4)\n",
        "}"
      ],
      "metadata": {
        "id": "38-Vuj-tqaWw"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xgb_bo(eta, gamma,max_depth, subsample ):\n",
        "  params={\n",
        "      'learning_rate' :max(eta, 0),\n",
        "      'gamma':int(round(gamma)),\n",
        "      'max_depth':int(round(max_depth)),\n",
        "      'subsample':int(round(subsample)),\n",
        "  }\n",
        "  model=XGBClassifier(**params, n_jobs=-1, random_state=42)\n",
        "  X_train,X_valid,y_train,y_valid=train_test_split(Xcc_train.iloc[:,1:],ycc_train,test_size=0.2)\n",
        "\n",
        "  model.fit(X_train,y_train)\n",
        "\n",
        "  score=f1_score(y_valid,model.predict(X_valid), average='micro')\n",
        "  return score"
      ],
      "metadata": {
        "id": "klM5EJZCqdYm"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BO_xgb = BayesianOptimization(f=xgb_bo,pbounds=xgb_params,random_state=3,verbose=2)\n",
        "\n",
        "\n",
        "# Bayesian Optimization을 실행해보세요\n",
        "BO_xgb.maximize(init_points=5,n_iter=100)\n",
        "xgb_max_params=BO_xgb.max['params']\n",
        "xgb_max_params['max_depth']=int(xgb_max_params['max_depth'])\n",
        "xgb_max_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCpioPDCqlYX",
        "outputId": "ecd00b41-b998-4dd6-8bdb-3484882dc9d0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |    eta    |   gamma   | max_depth | subsample |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.7959   \u001b[0m | \u001b[0m0.2208   \u001b[0m | \u001b[0m14.16    \u001b[0m | \u001b[0m582.5    \u001b[0m | \u001b[0m0.7554   \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m0.7857   \u001b[0m | \u001b[0m0.3573   \u001b[0m | \u001b[0m17.93    \u001b[0m | \u001b[0m252.0    \u001b[0m | \u001b[0m0.6036   \u001b[0m |\n",
            "| \u001b[95m3        \u001b[0m | \u001b[95m0.8418   \u001b[0m | \u001b[95m0.02154  \u001b[0m | \u001b[95m8.816    \u001b[0m | \u001b[95m60.72    \u001b[0m | \u001b[95m0.7284   \u001b[0m |\n",
            "| \u001b[95m4        \u001b[0m | \u001b[95m0.9082   \u001b[0m | \u001b[95m0.26     \u001b[0m | \u001b[95m5.57     \u001b[0m | \u001b[95m1.353e+03\u001b[0m | \u001b[95m0.7954   \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.8929   \u001b[0m | \u001b[0m0.01057  \u001b[0m | \u001b[0m11.18    \u001b[0m | \u001b[0m519.2    \u001b[0m | \u001b[0m0.7076   \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.8622   \u001b[0m | \u001b[0m0.008341 \u001b[0m | \u001b[0m4.807    \u001b[0m | \u001b[0m1.354e+03\u001b[0m | \u001b[0m0.8198   \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m0.8469   \u001b[0m | \u001b[0m0.1609   \u001b[0m | \u001b[0m6.579    \u001b[0m | \u001b[0m1.354e+03\u001b[0m | \u001b[0m0.9616   \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m0.8469   \u001b[0m | \u001b[0m0.365    \u001b[0m | \u001b[0m9.879    \u001b[0m | \u001b[0m518.3    \u001b[0m | \u001b[0m0.9369   \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.8622   \u001b[0m | \u001b[0m0.1397   \u001b[0m | \u001b[0m4.908    \u001b[0m | \u001b[0m1.352e+03\u001b[0m | \u001b[0m0.6803   \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m0.8929   \u001b[0m | \u001b[0m0.2606   \u001b[0m | \u001b[0m4.83     \u001b[0m | \u001b[0m1.353e+03\u001b[0m | \u001b[0m0.9735   \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m0.852    \u001b[0m | \u001b[0m0.1228   \u001b[0m | \u001b[0m11.23    \u001b[0m | \u001b[0m520.3    \u001b[0m | \u001b[0m0.7291   \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m0.8316   \u001b[0m | \u001b[0m0.1029   \u001b[0m | \u001b[0m13.26    \u001b[0m | \u001b[0m520.1    \u001b[0m | \u001b[0m0.6604   \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m0.8724   \u001b[0m | \u001b[0m0.3746   \u001b[0m | \u001b[0m6.291    \u001b[0m | \u001b[0m1.352e+03\u001b[0m | \u001b[0m0.7883   \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m0.801    \u001b[0m | \u001b[0m0.1085   \u001b[0m | \u001b[0m12.06    \u001b[0m | \u001b[0m518.0    \u001b[0m | \u001b[0m0.6035   \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m0.8418   \u001b[0m | \u001b[0m0.1243   \u001b[0m | \u001b[0m9.364    \u001b[0m | \u001b[0m520.3    \u001b[0m | \u001b[0m0.5354   \u001b[0m |\n",
            "| \u001b[0m16       \u001b[0m | \u001b[0m0.8622   \u001b[0m | \u001b[0m0.1768   \u001b[0m | \u001b[0m7.917    \u001b[0m | \u001b[0m1.352e+03\u001b[0m | \u001b[0m0.8724   \u001b[0m |\n",
            "| \u001b[0m17       \u001b[0m | \u001b[0m0.8316   \u001b[0m | \u001b[0m0.3896   \u001b[0m | \u001b[0m9.18     \u001b[0m | \u001b[0m1.352e+03\u001b[0m | \u001b[0m0.9705   \u001b[0m |\n",
            "| \u001b[0m18       \u001b[0m | \u001b[0m0.8724   \u001b[0m | \u001b[0m0.1207   \u001b[0m | \u001b[0m10.62    \u001b[0m | \u001b[0m519.6    \u001b[0m | \u001b[0m0.7731   \u001b[0m |\n",
            "| \u001b[0m19       \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m0.02944  \u001b[0m | \u001b[0m7.409    \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.7039   \u001b[0m |\n",
            "| \u001b[0m20       \u001b[0m | \u001b[0m0.8571   \u001b[0m | \u001b[0m0.3475   \u001b[0m | \u001b[0m8.015    \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.8635   \u001b[0m |\n",
            "| \u001b[0m21       \u001b[0m | \u001b[0m0.8724   \u001b[0m | \u001b[0m0.1719   \u001b[0m | \u001b[0m7.315    \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.7321   \u001b[0m |\n",
            "| \u001b[0m22       \u001b[0m | \u001b[0m0.8724   \u001b[0m | \u001b[0m0.1243   \u001b[0m | \u001b[0m5.982    \u001b[0m | \u001b[0m1.353e+03\u001b[0m | \u001b[0m0.5087   \u001b[0m |\n",
            "| \u001b[0m23       \u001b[0m | \u001b[0m0.7959   \u001b[0m | \u001b[0m0.3846   \u001b[0m | \u001b[0m6.869    \u001b[0m | \u001b[0m1.35e+03 \u001b[0m | \u001b[0m0.6048   \u001b[0m |\n",
            "| \u001b[0m24       \u001b[0m | \u001b[0m0.7857   \u001b[0m | \u001b[0m0.3007   \u001b[0m | \u001b[0m16.91    \u001b[0m | \u001b[0m1.553e+03\u001b[0m | \u001b[0m0.7851   \u001b[0m |\n",
            "| \u001b[0m25       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m0.1256   \u001b[0m | \u001b[0m2.841    \u001b[0m | \u001b[0m1.352e+03\u001b[0m | \u001b[0m0.9028   \u001b[0m |\n",
            "| \u001b[95m26       \u001b[0m | \u001b[95m0.9337   \u001b[0m | \u001b[95m0.3076   \u001b[0m | \u001b[95m3.118    \u001b[0m | \u001b[95m1.351e+03\u001b[0m | \u001b[95m0.9789   \u001b[0m |\n",
            "| \u001b[0m27       \u001b[0m | \u001b[0m0.8673   \u001b[0m | \u001b[0m0.3954   \u001b[0m | \u001b[0m3.708    \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.6571   \u001b[0m |\n",
            "| \u001b[0m28       \u001b[0m | \u001b[0m0.8929   \u001b[0m | \u001b[0m0.0812   \u001b[0m | \u001b[0m3.161    \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.8834   \u001b[0m |\n",
            "| \u001b[0m29       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m0.25     \u001b[0m | \u001b[0m0.179    \u001b[0m | \u001b[0m1.35e+03 \u001b[0m | \u001b[0m0.5554   \u001b[0m |\n",
            "| \u001b[0m30       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m0.03624  \u001b[0m | \u001b[0m1.166    \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.8855   \u001b[0m |\n",
            "| \u001b[0m31       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m0.01247  \u001b[0m | \u001b[0m1.356    \u001b[0m | \u001b[0m1.353e+03\u001b[0m | \u001b[0m0.8963   \u001b[0m |\n",
            "| \u001b[0m32       \u001b[0m | \u001b[0m0.8827   \u001b[0m | \u001b[0m0.218    \u001b[0m | \u001b[0m2.313    \u001b[0m | \u001b[0m1.353e+03\u001b[0m | \u001b[0m0.5357   \u001b[0m |\n",
            "| \u001b[0m33       \u001b[0m | \u001b[0m0.8724   \u001b[0m | \u001b[0m0.1371   \u001b[0m | \u001b[0m1.958    \u001b[0m | \u001b[0m1.35e+03 \u001b[0m | \u001b[0m0.971    \u001b[0m |\n",
            "| \u001b[0m34       \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m0.2421   \u001b[0m | \u001b[0m0.787    \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.8796   \u001b[0m |\n",
            "| \u001b[0m35       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m0.3388   \u001b[0m | \u001b[0m1.395    \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.6089   \u001b[0m |\n",
            "| \u001b[0m36       \u001b[0m | \u001b[0m0.8929   \u001b[0m | \u001b[0m0.02468  \u001b[0m | \u001b[0m1.451    \u001b[0m | \u001b[0m1.352e+03\u001b[0m | \u001b[0m0.7328   \u001b[0m |\n",
            "| \u001b[0m37       \u001b[0m | \u001b[0m0.898    \u001b[0m | \u001b[0m0.03015  \u001b[0m | \u001b[0m0.8051   \u001b[0m | \u001b[0m1.349e+03\u001b[0m | \u001b[0m0.61     \u001b[0m |\n",
            "| \u001b[0m38       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m0.3312   \u001b[0m | \u001b[0m0.3631   \u001b[0m | \u001b[0m1.353e+03\u001b[0m | \u001b[0m0.95     \u001b[0m |\n",
            "| \u001b[0m39       \u001b[0m | \u001b[0m0.9337   \u001b[0m | \u001b[0m0.2998   \u001b[0m | \u001b[0m1.346    \u001b[0m | \u001b[0m1.354e+03\u001b[0m | \u001b[0m0.8696   \u001b[0m |\n",
            "| \u001b[0m40       \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m0.06707  \u001b[0m | \u001b[0m1.54     \u001b[0m | \u001b[0m1.356e+03\u001b[0m | \u001b[0m0.7876   \u001b[0m |\n",
            "| \u001b[0m41       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m0.1318   \u001b[0m | \u001b[0m2.436    \u001b[0m | \u001b[0m1.356e+03\u001b[0m | \u001b[0m0.7401   \u001b[0m |\n",
            "| \u001b[95m42       \u001b[0m | \u001b[95m0.949    \u001b[0m | \u001b[95m0.2938   \u001b[0m | \u001b[95m0.8186   \u001b[0m | \u001b[95m1.354e+03\u001b[0m | \u001b[95m0.8094   \u001b[0m |\n",
            "| \u001b[0m43       \u001b[0m | \u001b[0m0.898    \u001b[0m | \u001b[0m0.01971  \u001b[0m | \u001b[0m1.249    \u001b[0m | \u001b[0m1.357e+03\u001b[0m | \u001b[0m0.6088   \u001b[0m |\n",
            "| \u001b[0m44       \u001b[0m | \u001b[0m0.8827   \u001b[0m | \u001b[0m0.009183 \u001b[0m | \u001b[0m0.3921   \u001b[0m | \u001b[0m1.354e+03\u001b[0m | \u001b[0m0.9651   \u001b[0m |\n",
            "| \u001b[0m45       \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m0.127    \u001b[0m | \u001b[0m3.012    \u001b[0m | \u001b[0m1.359e+03\u001b[0m | \u001b[0m0.7503   \u001b[0m |\n",
            "| \u001b[0m46       \u001b[0m | \u001b[0m0.8622   \u001b[0m | \u001b[0m0.2651   \u001b[0m | \u001b[0m3.637    \u001b[0m | \u001b[0m1.358e+03\u001b[0m | \u001b[0m0.5278   \u001b[0m |\n",
            "| \u001b[0m47       \u001b[0m | \u001b[0m0.8878   \u001b[0m | \u001b[0m0.1037   \u001b[0m | \u001b[0m1.196    \u001b[0m | \u001b[0m1.354e+03\u001b[0m | \u001b[0m0.5045   \u001b[0m |\n",
            "| \u001b[0m48       \u001b[0m | \u001b[0m0.898    \u001b[0m | \u001b[0m0.3167   \u001b[0m | \u001b[0m0.8001   \u001b[0m | \u001b[0m1.352e+03\u001b[0m | \u001b[0m0.7028   \u001b[0m |\n",
            "| \u001b[0m49       \u001b[0m | \u001b[0m0.949    \u001b[0m | \u001b[0m0.1972   \u001b[0m | \u001b[0m0.697    \u001b[0m | \u001b[0m1.35e+03 \u001b[0m | \u001b[0m0.6365   \u001b[0m |\n",
            "| \u001b[0m50       \u001b[0m | \u001b[0m0.9388   \u001b[0m | \u001b[0m0.3207   \u001b[0m | \u001b[0m1.873    \u001b[0m | \u001b[0m1.359e+03\u001b[0m | \u001b[0m0.9586   \u001b[0m |\n",
            "| \u001b[0m51       \u001b[0m | \u001b[0m0.8418   \u001b[0m | \u001b[0m0.00484  \u001b[0m | \u001b[0m2.197    \u001b[0m | \u001b[0m1.359e+03\u001b[0m | \u001b[0m0.6285   \u001b[0m |\n",
            "| \u001b[0m52       \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m0.2896   \u001b[0m | \u001b[0m5.422    \u001b[0m | \u001b[0m1.353e+03\u001b[0m | \u001b[0m0.888    \u001b[0m |\n",
            "| \u001b[95m53       \u001b[0m | \u001b[95m0.9541   \u001b[0m | \u001b[95m0.3423   \u001b[0m | \u001b[95m0.3462   \u001b[0m | \u001b[95m1.351e+03\u001b[0m | \u001b[95m0.703    \u001b[0m |\n",
            "| \u001b[0m54       \u001b[0m | \u001b[0m0.8571   \u001b[0m | \u001b[0m0.08928  \u001b[0m | \u001b[0m3.308    \u001b[0m | \u001b[0m1.356e+03\u001b[0m | \u001b[0m0.6046   \u001b[0m |\n",
            "| \u001b[0m55       \u001b[0m | \u001b[0m0.8571   \u001b[0m | \u001b[0m0.1795   \u001b[0m | \u001b[0m8.38     \u001b[0m | \u001b[0m1.398e+03\u001b[0m | \u001b[0m0.8724   \u001b[0m |\n",
            "| \u001b[0m56       \u001b[0m | \u001b[0m0.8673   \u001b[0m | \u001b[0m0.3471   \u001b[0m | \u001b[0m0.05465  \u001b[0m | \u001b[0m1.361e+03\u001b[0m | \u001b[0m0.6631   \u001b[0m |\n",
            "| \u001b[0m57       \u001b[0m | \u001b[0m0.7704   \u001b[0m | \u001b[0m0.289    \u001b[0m | \u001b[0m14.04    \u001b[0m | \u001b[0m1.167e+03\u001b[0m | \u001b[0m0.6752   \u001b[0m |\n",
            "| \u001b[0m58       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m0.2995   \u001b[0m | \u001b[0m0.735    \u001b[0m | \u001b[0m1.355e+03\u001b[0m | \u001b[0m0.8257   \u001b[0m |\n",
            "| \u001b[0m59       \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m0.1793   \u001b[0m | \u001b[0m1.041    \u001b[0m | \u001b[0m1.359e+03\u001b[0m | \u001b[0m0.73     \u001b[0m |\n",
            "| \u001b[0m60       \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m0.02644  \u001b[0m | \u001b[0m1.339    \u001b[0m | \u001b[0m1.355e+03\u001b[0m | \u001b[0m0.6332   \u001b[0m |\n",
            "| \u001b[0m61       \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m0.2993   \u001b[0m | \u001b[0m1.496    \u001b[0m | \u001b[0m1.358e+03\u001b[0m | \u001b[0m0.8454   \u001b[0m |\n",
            "| \u001b[0m62       \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m0.1637   \u001b[0m | \u001b[0m0.01604  \u001b[0m | \u001b[0m1.357e+03\u001b[0m | \u001b[0m0.5852   \u001b[0m |\n",
            "| \u001b[0m63       \u001b[0m | \u001b[0m0.9337   \u001b[0m | \u001b[0m0.1966   \u001b[0m | \u001b[0m0.01157  \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.884    \u001b[0m |\n",
            "| \u001b[0m64       \u001b[0m | \u001b[0m0.9541   \u001b[0m | \u001b[0m0.3199   \u001b[0m | \u001b[0m0.2901   \u001b[0m | \u001b[0m1.35e+03 \u001b[0m | \u001b[0m0.9889   \u001b[0m |\n",
            "| \u001b[0m65       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m0.03935  \u001b[0m | \u001b[0m1.118    \u001b[0m | \u001b[0m1.36e+03 \u001b[0m | \u001b[0m0.9658   \u001b[0m |\n",
            "| \u001b[0m66       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m0.1935   \u001b[0m | \u001b[0m2.298    \u001b[0m | \u001b[0m1.357e+03\u001b[0m | \u001b[0m0.5368   \u001b[0m |\n",
            "| \u001b[0m67       \u001b[0m | \u001b[0m0.8878   \u001b[0m | \u001b[0m0.1171   \u001b[0m | \u001b[0m2.235    \u001b[0m | \u001b[0m1.355e+03\u001b[0m | \u001b[0m0.914    \u001b[0m |\n",
            "| \u001b[0m68       \u001b[0m | \u001b[0m0.8776   \u001b[0m | \u001b[0m0.2396   \u001b[0m | \u001b[0m9.381    \u001b[0m | \u001b[0m1.052e+03\u001b[0m | \u001b[0m0.8804   \u001b[0m |\n",
            "| \u001b[0m69       \u001b[0m | \u001b[0m0.8878   \u001b[0m | \u001b[0m0.1909   \u001b[0m | \u001b[0m0.5076   \u001b[0m | \u001b[0m1.347e+03\u001b[0m | \u001b[0m0.9178   \u001b[0m |\n",
            "| \u001b[0m70       \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m0.3634   \u001b[0m | \u001b[0m0.7513   \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.5833   \u001b[0m |\n",
            "| \u001b[0m71       \u001b[0m | \u001b[0m0.7602   \u001b[0m | \u001b[0m0.2668   \u001b[0m | \u001b[0m16.53    \u001b[0m | \u001b[0m297.3    \u001b[0m | \u001b[0m0.554    \u001b[0m |\n",
            "| \u001b[0m72       \u001b[0m | \u001b[0m0.8827   \u001b[0m | \u001b[0m0.05479  \u001b[0m | \u001b[0m0.5921   \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.8415   \u001b[0m |\n",
            "| \u001b[0m73       \u001b[0m | \u001b[0m0.9388   \u001b[0m | \u001b[0m0.2063   \u001b[0m | \u001b[0m3.005    \u001b[0m | \u001b[0m1.346e+03\u001b[0m | \u001b[0m0.8334   \u001b[0m |\n",
            "| \u001b[0m74       \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m0.2529   \u001b[0m | \u001b[0m1.361    \u001b[0m | \u001b[0m1.358e+03\u001b[0m | \u001b[0m0.7757   \u001b[0m |\n",
            "| \u001b[0m75       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m0.1749   \u001b[0m | \u001b[0m0.3551   \u001b[0m | \u001b[0m1.403e+03\u001b[0m | \u001b[0m0.8597   \u001b[0m |\n",
            "| \u001b[0m76       \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m0.2684   \u001b[0m | \u001b[0m3.889    \u001b[0m | \u001b[0m1.346e+03\u001b[0m | \u001b[0m0.7224   \u001b[0m |\n",
            "| \u001b[0m77       \u001b[0m | \u001b[0m0.8622   \u001b[0m | \u001b[0m0.3339   \u001b[0m | \u001b[0m5.582    \u001b[0m | \u001b[0m545.6    \u001b[0m | \u001b[0m0.5914   \u001b[0m |\n",
            "| \u001b[0m78       \u001b[0m | \u001b[0m0.8827   \u001b[0m | \u001b[0m0.1481   \u001b[0m | \u001b[0m7.509    \u001b[0m | \u001b[0m424.6    \u001b[0m | \u001b[0m0.853    \u001b[0m |\n",
            "| \u001b[0m79       \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m0.3708   \u001b[0m | \u001b[0m2.328    \u001b[0m | \u001b[0m1.346e+03\u001b[0m | \u001b[0m0.5322   \u001b[0m |\n",
            "| \u001b[0m80       \u001b[0m | \u001b[0m0.9388   \u001b[0m | \u001b[0m0.3211   \u001b[0m | \u001b[0m0.1073   \u001b[0m | \u001b[0m1.352e+03\u001b[0m | \u001b[0m0.893    \u001b[0m |\n",
            "| \u001b[0m81       \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m0.04583  \u001b[0m | \u001b[0m1.403    \u001b[0m | \u001b[0m904.4    \u001b[0m | \u001b[0m0.641    \u001b[0m |\n",
            "| \u001b[0m82       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m0.1321   \u001b[0m | \u001b[0m3.011    \u001b[0m | \u001b[0m1.359e+03\u001b[0m | \u001b[0m0.6037   \u001b[0m |\n",
            "| \u001b[0m83       \u001b[0m | \u001b[0m0.7449   \u001b[0m | \u001b[0m0.2942   \u001b[0m | \u001b[0m14.94    \u001b[0m | \u001b[0m696.2    \u001b[0m | \u001b[0m0.5203   \u001b[0m |\n",
            "| \u001b[0m84       \u001b[0m | \u001b[0m0.949    \u001b[0m | \u001b[0m0.206    \u001b[0m | \u001b[0m1.387    \u001b[0m | \u001b[0m1.355e+03\u001b[0m | \u001b[0m0.6616   \u001b[0m |\n",
            "| \u001b[0m85       \u001b[0m | \u001b[0m0.9439   \u001b[0m | \u001b[0m0.1115   \u001b[0m | \u001b[0m0.1969   \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.8521   \u001b[0m |\n",
            "| \u001b[0m86       \u001b[0m | \u001b[0m0.898    \u001b[0m | \u001b[0m0.3448   \u001b[0m | \u001b[0m7.988    \u001b[0m | \u001b[0m1.181e+03\u001b[0m | \u001b[0m0.8652   \u001b[0m |\n",
            "| \u001b[0m87       \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m0.1979   \u001b[0m | \u001b[0m4.19     \u001b[0m | \u001b[0m613.1    \u001b[0m | \u001b[0m0.8      \u001b[0m |\n",
            "| \u001b[0m88       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m0.04244  \u001b[0m | \u001b[0m1.036    \u001b[0m | \u001b[0m987.6    \u001b[0m | \u001b[0m0.6037   \u001b[0m |\n",
            "| \u001b[0m89       \u001b[0m | \u001b[0m0.898    \u001b[0m | \u001b[0m0.0665   \u001b[0m | \u001b[0m0.05554  \u001b[0m | \u001b[0m1.403e+03\u001b[0m | \u001b[0m0.5091   \u001b[0m |\n",
            "| \u001b[0m90       \u001b[0m | \u001b[0m0.8929   \u001b[0m | \u001b[0m0.1923   \u001b[0m | \u001b[0m2.362    \u001b[0m | \u001b[0m1.355e+03\u001b[0m | \u001b[0m0.704    \u001b[0m |\n",
            "| \u001b[0m91       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m0.1129   \u001b[0m | \u001b[0m3.417    \u001b[0m | \u001b[0m1.347e+03\u001b[0m | \u001b[0m0.5304   \u001b[0m |\n",
            "| \u001b[0m92       \u001b[0m | \u001b[0m0.8776   \u001b[0m | \u001b[0m0.005542 \u001b[0m | \u001b[0m2.069    \u001b[0m | \u001b[0m987.7    \u001b[0m | \u001b[0m0.9604   \u001b[0m |\n",
            "| \u001b[0m93       \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m0.02747  \u001b[0m | \u001b[0m1.797    \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m0.9619   \u001b[0m |\n",
            "| \u001b[0m94       \u001b[0m | \u001b[0m0.9337   \u001b[0m | \u001b[0m0.2159   \u001b[0m | \u001b[0m0.06533  \u001b[0m | \u001b[0m1.349e+03\u001b[0m | \u001b[0m0.9944   \u001b[0m |\n",
            "| \u001b[0m95       \u001b[0m | \u001b[0m0.8776   \u001b[0m | \u001b[0m0.1574   \u001b[0m | \u001b[0m1.127    \u001b[0m | \u001b[0m1.35e+03 \u001b[0m | \u001b[0m0.7131   \u001b[0m |\n",
            "| \u001b[0m96       \u001b[0m | \u001b[0m0.8367   \u001b[0m | \u001b[0m0.255    \u001b[0m | \u001b[0m13.69    \u001b[0m | \u001b[0m758.2    \u001b[0m | \u001b[0m0.956    \u001b[0m |\n",
            "| \u001b[0m97       \u001b[0m | \u001b[0m0.8673   \u001b[0m | \u001b[0m0.34     \u001b[0m | \u001b[0m7.39     \u001b[0m | \u001b[0m1.389e+03\u001b[0m | \u001b[0m0.5526   \u001b[0m |\n",
            "| \u001b[0m98       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m0.3398   \u001b[0m | \u001b[0m5.129    \u001b[0m | \u001b[0m1.565e+03\u001b[0m | \u001b[0m0.8592   \u001b[0m |\n",
            "| \u001b[0m99       \u001b[0m | \u001b[0m0.852    \u001b[0m | \u001b[0m0.3841   \u001b[0m | \u001b[0m9.514    \u001b[0m | \u001b[0m1.133e+03\u001b[0m | \u001b[0m0.5409   \u001b[0m |\n",
            "| \u001b[0m100      \u001b[0m | \u001b[0m0.8827   \u001b[0m | \u001b[0m0.3738   \u001b[0m | \u001b[0m1.67     \u001b[0m | \u001b[0m1.309e+03\u001b[0m | \u001b[0m0.6532   \u001b[0m |\n",
            "| \u001b[0m101      \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m0.122    \u001b[0m | \u001b[0m0.01624  \u001b[0m | \u001b[0m1.35e+03 \u001b[0m | \u001b[0m0.8358   \u001b[0m |\n",
            "| \u001b[0m102      \u001b[0m | \u001b[0m0.8929   \u001b[0m | \u001b[0m0.3654   \u001b[0m | \u001b[0m4.366    \u001b[0m | \u001b[0m572.7    \u001b[0m | \u001b[0m0.5214   \u001b[0m |\n",
            "| \u001b[0m103      \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m0.1799   \u001b[0m | \u001b[0m1.796    \u001b[0m | \u001b[0m1.356e+03\u001b[0m | \u001b[0m0.8605   \u001b[0m |\n",
            "| \u001b[0m104      \u001b[0m | \u001b[0m0.898    \u001b[0m | \u001b[0m0.3175   \u001b[0m | \u001b[0m3.233    \u001b[0m | \u001b[0m1.345e+03\u001b[0m | \u001b[0m0.9671   \u001b[0m |\n",
            "| \u001b[0m105      \u001b[0m | \u001b[0m0.7398   \u001b[0m | \u001b[0m0.1652   \u001b[0m | \u001b[0m16.16    \u001b[0m | \u001b[0m1.125e+03\u001b[0m | \u001b[0m0.5941   \u001b[0m |\n",
            "=========================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eta': 0.3423466837492939,\n",
              " 'gamma': 0.3462431050442949,\n",
              " 'max_depth': 1351,\n",
              " 'subsample': 0.7029948366468876}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_params={\n",
        "    'n_estimators':(30,100),\n",
        "    'max_depth':(1,2000),\n",
        "    'subsample':(0.5,1)\n",
        "}"
      ],
      "metadata": {
        "id": "aYQ1Ge74q4_3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lgbm_bo(n_estimators,max_depth, subsample):\n",
        "  params={\n",
        "      'n_estimaotrs':int(round(n_estimators)),\n",
        "      'max_depth':int(round(max_depth)),\n",
        "      'subsample':int(round(subsample)),\n",
        "  }\n",
        "  model=LGBMClassifier(**params, n_jobs=-1, random_state=42)\n",
        "  X_train,X_valid,y_train,y_valid=train_test_split(Xcc_train.iloc[:,1:],ycc_train,test_size=0.2)\n",
        "  model.fit(X_train,y_train)\n",
        "\n",
        "  score=f1_score(y_valid,model.predict(X_valid),average='micro')\n",
        "  return score"
      ],
      "metadata": {
        "id": "9W0vvM9oq8tg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BO_lgbm = BayesianOptimization(f=lgbm_bo,pbounds=lgbm_params,random_state=3,verbose=2)\n",
        "\n",
        "\n",
        "# Bayesian Optimization을 실행해보세요\n",
        "BO_lgbm.maximize(init_points=5,n_iter=100)\n",
        "lgbm_max_params=BO_lgbm.max['params']\n",
        "lgbm_max_params['n_estimators']=int(lgbm_max_params['n_estimators'])\n",
        "lgbm_max_params['max_depth']=int(lgbm_max_params['max_depth'])\n",
        "lgbm_max_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhWQa2s6rFkN",
        "outputId": "b35cb2b7-99a2-4836-dc21-2af66c50e748"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | max_depth | n_esti... | subsample |\n",
            "-------------------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m1.102e+03\u001b[0m | \u001b[0m79.57    \u001b[0m | \u001b[0m0.6455   \u001b[0m |\n",
            "| \u001b[95m2        \u001b[0m | \u001b[95m0.949    \u001b[0m | \u001b[95m1.022e+03\u001b[0m | \u001b[95m92.51    \u001b[0m | \u001b[95m0.9481   \u001b[0m |\n",
            "| \u001b[95m3        \u001b[0m | \u001b[95m0.9592   \u001b[0m | \u001b[95m252.0    \u001b[0m | \u001b[95m44.51    \u001b[0m | \u001b[95m0.5257   \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m882.2    \u001b[0m | \u001b[0m32.09    \u001b[0m | \u001b[0m0.7284   \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.9541   \u001b[0m | \u001b[0m1.299e+03\u001b[0m | \u001b[0m49.49    \u001b[0m | \u001b[0m0.8381   \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.9337   \u001b[0m | \u001b[0m251.4    \u001b[0m | \u001b[0m42.56    \u001b[0m | \u001b[0m0.6701   \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m1.416e+03\u001b[0m | \u001b[0m77.83    \u001b[0m | \u001b[0m0.9654   \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m0.9439   \u001b[0m | \u001b[0m75.94    \u001b[0m | \u001b[0m72.97    \u001b[0m | \u001b[0m0.9886   \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.898    \u001b[0m | \u001b[0m1.726e+03\u001b[0m | \u001b[0m58.46    \u001b[0m | \u001b[0m0.651    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m1.024e+03\u001b[0m | \u001b[0m92.96    \u001b[0m | \u001b[0m0.7399   \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m344.6    \u001b[0m | \u001b[0m51.65    \u001b[0m | \u001b[0m0.5972   \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m0.949    \u001b[0m | \u001b[0m251.5    \u001b[0m | \u001b[0m42.0     \u001b[0m | \u001b[0m0.5055   \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m251.1    \u001b[0m | \u001b[0m41.45    \u001b[0m | \u001b[0m0.9826   \u001b[0m |\n",
            "| \u001b[95m14       \u001b[0m | \u001b[95m0.9694   \u001b[0m | \u001b[95m1.541e+03\u001b[0m | \u001b[95m31.59    \u001b[0m | \u001b[95m0.7545   \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m1.223e+03\u001b[0m | \u001b[0m96.1     \u001b[0m | \u001b[0m0.9014   \u001b[0m |\n",
            "| \u001b[0m16       \u001b[0m | \u001b[0m0.949    \u001b[0m | \u001b[0m1.542e+03\u001b[0m | \u001b[0m31.24    \u001b[0m | \u001b[0m0.8932   \u001b[0m |\n",
            "| \u001b[0m17       \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m1.767e+03\u001b[0m | \u001b[0m35.48    \u001b[0m | \u001b[0m0.8069   \u001b[0m |\n",
            "| \u001b[0m18       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m1.541e+03\u001b[0m | \u001b[0m32.51    \u001b[0m | \u001b[0m0.7066   \u001b[0m |\n",
            "| \u001b[0m19       \u001b[0m | \u001b[0m0.9439   \u001b[0m | \u001b[0m1.299e+03\u001b[0m | \u001b[0m48.49    \u001b[0m | \u001b[0m0.8679   \u001b[0m |\n",
            "| \u001b[0m20       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m252.5    \u001b[0m | \u001b[0m42.2     \u001b[0m | \u001b[0m0.9536   \u001b[0m |\n",
            "| \u001b[0m21       \u001b[0m | \u001b[0m0.9388   \u001b[0m | \u001b[0m1.299e+03\u001b[0m | \u001b[0m50.98    \u001b[0m | \u001b[0m0.7878   \u001b[0m |\n",
            "| \u001b[0m22       \u001b[0m | \u001b[0m0.9592   \u001b[0m | \u001b[0m1.299e+03\u001b[0m | \u001b[0m49.59    \u001b[0m | \u001b[0m0.8491   \u001b[0m |\n",
            "| \u001b[0m23       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m1.031e+03\u001b[0m | \u001b[0m34.88    \u001b[0m | \u001b[0m0.707    \u001b[0m |\n",
            "| \u001b[0m24       \u001b[0m | \u001b[0m0.949    \u001b[0m | \u001b[0m528.9    \u001b[0m | \u001b[0m64.3     \u001b[0m | \u001b[0m0.6772   \u001b[0m |\n",
            "| \u001b[0m25       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m97.7     \u001b[0m | \u001b[0m32.09    \u001b[0m | \u001b[0m0.8915   \u001b[0m |\n",
            "| \u001b[0m26       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m251.4    \u001b[0m | \u001b[0m42.74    \u001b[0m | \u001b[0m0.5711   \u001b[0m |\n",
            "| \u001b[0m27       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m1.301e+03\u001b[0m | \u001b[0m48.66    \u001b[0m | \u001b[0m0.9474   \u001b[0m |\n",
            "| \u001b[0m28       \u001b[0m | \u001b[0m0.9337   \u001b[0m | \u001b[0m76.83    \u001b[0m | \u001b[0m74.57    \u001b[0m | \u001b[0m0.8955   \u001b[0m |\n",
            "| \u001b[0m29       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m251.7    \u001b[0m | \u001b[0m45.89    \u001b[0m | \u001b[0m0.8986   \u001b[0m |\n",
            "| \u001b[0m30       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m1.541e+03\u001b[0m | \u001b[0m30.82    \u001b[0m | \u001b[0m0.6906   \u001b[0m |\n",
            "| \u001b[0m31       \u001b[0m | \u001b[0m0.9541   \u001b[0m | \u001b[0m543.1    \u001b[0m | \u001b[0m52.38    \u001b[0m | \u001b[0m0.9954   \u001b[0m |\n",
            "| \u001b[0m32       \u001b[0m | \u001b[0m0.9388   \u001b[0m | \u001b[0m1.748e+03\u001b[0m | \u001b[0m50.77    \u001b[0m | \u001b[0m0.9984   \u001b[0m |\n",
            "| \u001b[0m33       \u001b[0m | \u001b[0m0.9541   \u001b[0m | \u001b[0m7.056    \u001b[0m | \u001b[0m60.45    \u001b[0m | \u001b[0m0.7409   \u001b[0m |\n",
            "| \u001b[0m34       \u001b[0m | \u001b[0m0.9388   \u001b[0m | \u001b[0m466.4    \u001b[0m | \u001b[0m67.3     \u001b[0m | \u001b[0m0.7736   \u001b[0m |\n",
            "| \u001b[0m35       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m1.54e+03 \u001b[0m | \u001b[0m32.02    \u001b[0m | \u001b[0m0.7063   \u001b[0m |\n",
            "| \u001b[0m36       \u001b[0m | \u001b[0m0.949    \u001b[0m | \u001b[0m612.9    \u001b[0m | \u001b[0m70.91    \u001b[0m | \u001b[0m0.6441   \u001b[0m |\n",
            "| \u001b[0m37       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m529.4    \u001b[0m | \u001b[0m63.83    \u001b[0m | \u001b[0m0.7104   \u001b[0m |\n",
            "| \u001b[0m38       \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m251.5    \u001b[0m | \u001b[0m41.86    \u001b[0m | \u001b[0m0.9264   \u001b[0m |\n",
            "| \u001b[0m39       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m6.777    \u001b[0m | \u001b[0m60.6     \u001b[0m | \u001b[0m0.9951   \u001b[0m |\n",
            "| \u001b[0m40       \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m251.1    \u001b[0m | \u001b[0m42.14    \u001b[0m | \u001b[0m0.6182   \u001b[0m |\n",
            "| \u001b[0m41       \u001b[0m | \u001b[0m0.9439   \u001b[0m | \u001b[0m1.857e+03\u001b[0m | \u001b[0m52.64    \u001b[0m | \u001b[0m0.9375   \u001b[0m |\n",
            "| \u001b[0m42       \u001b[0m | \u001b[0m0.9439   \u001b[0m | \u001b[0m1.316e+03\u001b[0m | \u001b[0m62.5     \u001b[0m | \u001b[0m0.7069   \u001b[0m |\n",
            "| \u001b[0m43       \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m609.1    \u001b[0m | \u001b[0m69.11    \u001b[0m | \u001b[0m0.804    \u001b[0m |\n",
            "| \u001b[0m44       \u001b[0m | \u001b[0m0.9439   \u001b[0m | \u001b[0m787.1    \u001b[0m | \u001b[0m74.74    \u001b[0m | \u001b[0m0.8491   \u001b[0m |\n",
            "| \u001b[0m45       \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m1.086e+03\u001b[0m | \u001b[0m55.34    \u001b[0m | \u001b[0m0.5159   \u001b[0m |\n",
            "| \u001b[0m46       \u001b[0m | \u001b[0m0.9541   \u001b[0m | \u001b[0m465.8    \u001b[0m | \u001b[0m51.76    \u001b[0m | \u001b[0m0.996    \u001b[0m |\n",
            "| \u001b[0m47       \u001b[0m | \u001b[0m0.9592   \u001b[0m | \u001b[0m1.298e+03\u001b[0m | \u001b[0m50.82    \u001b[0m | \u001b[0m0.7847   \u001b[0m |\n",
            "| \u001b[0m48       \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m251.7    \u001b[0m | \u001b[0m45.12    \u001b[0m | \u001b[0m0.6255   \u001b[0m |\n",
            "| \u001b[0m49       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m1.022e+03\u001b[0m | \u001b[0m92.14    \u001b[0m | \u001b[0m0.7252   \u001b[0m |\n",
            "| \u001b[0m50       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m664.6    \u001b[0m | \u001b[0m32.27    \u001b[0m | \u001b[0m0.5613   \u001b[0m |\n",
            "| \u001b[0m51       \u001b[0m | \u001b[0m0.9592   \u001b[0m | \u001b[0m1.669e+03\u001b[0m | \u001b[0m35.36    \u001b[0m | \u001b[0m0.6778   \u001b[0m |\n",
            "| \u001b[0m52       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m1.857e+03\u001b[0m | \u001b[0m52.72    \u001b[0m | \u001b[0m0.9546   \u001b[0m |\n",
            "| \u001b[0m53       \u001b[0m | \u001b[0m0.9592   \u001b[0m | \u001b[0m263.0    \u001b[0m | \u001b[0m78.82    \u001b[0m | \u001b[0m0.9971   \u001b[0m |\n",
            "| \u001b[0m54       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m97.31    \u001b[0m | \u001b[0m76.47    \u001b[0m | \u001b[0m0.7446   \u001b[0m |\n",
            "| \u001b[0m55       \u001b[0m | \u001b[0m0.949    \u001b[0m | \u001b[0m979.2    \u001b[0m | \u001b[0m85.34    \u001b[0m | \u001b[0m0.555    \u001b[0m |\n",
            "| \u001b[0m56       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m322.8    \u001b[0m | \u001b[0m70.16    \u001b[0m | \u001b[0m0.519    \u001b[0m |\n",
            "| \u001b[0m57       \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m1.553e+03\u001b[0m | \u001b[0m88.63    \u001b[0m | \u001b[0m0.5806   \u001b[0m |\n",
            "| \u001b[0m58       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m239.4    \u001b[0m | \u001b[0m41.37    \u001b[0m | \u001b[0m0.8762   \u001b[0m |\n",
            "| \u001b[0m59       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m1.949e+03\u001b[0m | \u001b[0m61.73    \u001b[0m | \u001b[0m0.5417   \u001b[0m |\n",
            "| \u001b[0m60       \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m1.351e+03\u001b[0m | \u001b[0m93.42    \u001b[0m | \u001b[0m0.8922   \u001b[0m |\n",
            "| \u001b[0m61       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m12.17    \u001b[0m | \u001b[0m81.24    \u001b[0m | \u001b[0m0.6056   \u001b[0m |\n",
            "| \u001b[0m62       \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m335.3    \u001b[0m | \u001b[0m38.68    \u001b[0m | \u001b[0m0.5223   \u001b[0m |\n",
            "| \u001b[0m63       \u001b[0m | \u001b[0m0.9592   \u001b[0m | \u001b[0m1.005e+03\u001b[0m | \u001b[0m65.79    \u001b[0m | \u001b[0m0.5532   \u001b[0m |\n",
            "| \u001b[0m64       \u001b[0m | \u001b[0m0.9439   \u001b[0m | \u001b[0m624.4    \u001b[0m | \u001b[0m50.36    \u001b[0m | \u001b[0m0.6586   \u001b[0m |\n",
            "| \u001b[0m65       \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m385.0    \u001b[0m | \u001b[0m55.04    \u001b[0m | \u001b[0m0.6949   \u001b[0m |\n",
            "| \u001b[0m66       \u001b[0m | \u001b[0m0.9388   \u001b[0m | \u001b[0m309.3    \u001b[0m | \u001b[0m47.58    \u001b[0m | \u001b[0m0.8942   \u001b[0m |\n",
            "| \u001b[0m67       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m1.652e+03\u001b[0m | \u001b[0m73.41    \u001b[0m | \u001b[0m0.7261   \u001b[0m |\n",
            "| \u001b[0m68       \u001b[0m | \u001b[0m0.9388   \u001b[0m | \u001b[0m80.77    \u001b[0m | \u001b[0m63.61    \u001b[0m | \u001b[0m0.6879   \u001b[0m |\n",
            "| \u001b[0m69       \u001b[0m | \u001b[0m0.9439   \u001b[0m | \u001b[0m1.319e+03\u001b[0m | \u001b[0m89.63    \u001b[0m | \u001b[0m0.5484   \u001b[0m |\n",
            "| \u001b[0m70       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m204.9    \u001b[0m | \u001b[0m48.32    \u001b[0m | \u001b[0m0.6894   \u001b[0m |\n",
            "| \u001b[0m71       \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m1.098e+03\u001b[0m | \u001b[0m65.61    \u001b[0m | \u001b[0m0.6854   \u001b[0m |\n",
            "| \u001b[0m72       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m1.98e+03 \u001b[0m | \u001b[0m51.84    \u001b[0m | \u001b[0m0.5886   \u001b[0m |\n",
            "| \u001b[0m73       \u001b[0m | \u001b[0m0.9337   \u001b[0m | \u001b[0m587.8    \u001b[0m | \u001b[0m32.96    \u001b[0m | \u001b[0m0.7674   \u001b[0m |\n",
            "| \u001b[0m74       \u001b[0m | \u001b[0m0.9031   \u001b[0m | \u001b[0m1.739e+03\u001b[0m | \u001b[0m67.09    \u001b[0m | \u001b[0m0.5221   \u001b[0m |\n",
            "| \u001b[0m75       \u001b[0m | \u001b[0m0.9439   \u001b[0m | \u001b[0m476.5    \u001b[0m | \u001b[0m58.4     \u001b[0m | \u001b[0m0.7934   \u001b[0m |\n",
            "| \u001b[0m76       \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m1.272e+03\u001b[0m | \u001b[0m51.17    \u001b[0m | \u001b[0m0.5079   \u001b[0m |\n",
            "| \u001b[0m77       \u001b[0m | \u001b[0m0.8929   \u001b[0m | \u001b[0m200.0    \u001b[0m | \u001b[0m96.2     \u001b[0m | \u001b[0m0.8287   \u001b[0m |\n",
            "| \u001b[0m78       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m1.358e+03\u001b[0m | \u001b[0m95.01    \u001b[0m | \u001b[0m0.5973   \u001b[0m |\n",
            "| \u001b[0m79       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m1.91e+03 \u001b[0m | \u001b[0m93.92    \u001b[0m | \u001b[0m0.9995   \u001b[0m |\n",
            "| \u001b[0m80       \u001b[0m | \u001b[0m0.898    \u001b[0m | \u001b[0m113.2    \u001b[0m | \u001b[0m72.89    \u001b[0m | \u001b[0m0.8431   \u001b[0m |\n",
            "| \u001b[0m81       \u001b[0m | \u001b[0m0.9337   \u001b[0m | \u001b[0m422.9    \u001b[0m | \u001b[0m30.99    \u001b[0m | \u001b[0m0.9815   \u001b[0m |\n",
            "| \u001b[0m82       \u001b[0m | \u001b[0m0.9337   \u001b[0m | \u001b[0m691.4    \u001b[0m | \u001b[0m52.09    \u001b[0m | \u001b[0m0.7751   \u001b[0m |\n",
            "| \u001b[0m83       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m875.0    \u001b[0m | \u001b[0m63.73    \u001b[0m | \u001b[0m0.6153   \u001b[0m |\n",
            "| \u001b[0m84       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m587.3    \u001b[0m | \u001b[0m77.01    \u001b[0m | \u001b[0m0.9163   \u001b[0m |\n",
            "| \u001b[0m85       \u001b[0m | \u001b[0m0.9541   \u001b[0m | \u001b[0m918.3    \u001b[0m | \u001b[0m40.09    \u001b[0m | \u001b[0m0.8808   \u001b[0m |\n",
            "| \u001b[0m86       \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m855.6    \u001b[0m | \u001b[0m32.19    \u001b[0m | \u001b[0m0.6944   \u001b[0m |\n",
            "| \u001b[0m87       \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m1.352e+03\u001b[0m | \u001b[0m70.87    \u001b[0m | \u001b[0m0.7313   \u001b[0m |\n",
            "| \u001b[0m88       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m1.417e+03\u001b[0m | \u001b[0m30.2     \u001b[0m | \u001b[0m0.968    \u001b[0m |\n",
            "| \u001b[0m89       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m483.9    \u001b[0m | \u001b[0m94.24    \u001b[0m | \u001b[0m0.7629   \u001b[0m |\n",
            "| \u001b[0m90       \u001b[0m | \u001b[0m0.9541   \u001b[0m | \u001b[0m1.485e+03\u001b[0m | \u001b[0m42.84    \u001b[0m | \u001b[0m0.6254   \u001b[0m |\n",
            "| \u001b[0m91       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m351.3    \u001b[0m | \u001b[0m55.95    \u001b[0m | \u001b[0m0.9519   \u001b[0m |\n",
            "| \u001b[0m92       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m942.0    \u001b[0m | \u001b[0m35.87    \u001b[0m | \u001b[0m0.6531   \u001b[0m |\n",
            "| \u001b[0m93       \u001b[0m | \u001b[0m0.9235   \u001b[0m | \u001b[0m36.77    \u001b[0m | \u001b[0m77.71    \u001b[0m | \u001b[0m0.9106   \u001b[0m |\n",
            "| \u001b[0m94       \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m1.047e+03\u001b[0m | \u001b[0m97.64    \u001b[0m | \u001b[0m0.6466   \u001b[0m |\n",
            "| \u001b[0m95       \u001b[0m | \u001b[0m0.9184   \u001b[0m | \u001b[0m1.423e+03\u001b[0m | \u001b[0m97.73    \u001b[0m | \u001b[0m0.9083   \u001b[0m |\n",
            "| \u001b[0m96       \u001b[0m | \u001b[0m0.9388   \u001b[0m | \u001b[0m1.764e+03\u001b[0m | \u001b[0m41.9     \u001b[0m | \u001b[0m0.652    \u001b[0m |\n",
            "| \u001b[0m97       \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m230.1    \u001b[0m | \u001b[0m48.61    \u001b[0m | \u001b[0m0.8231   \u001b[0m |\n",
            "| \u001b[0m98       \u001b[0m | \u001b[0m0.949    \u001b[0m | \u001b[0m834.9    \u001b[0m | \u001b[0m73.16    \u001b[0m | \u001b[0m0.8943   \u001b[0m |\n",
            "| \u001b[0m99       \u001b[0m | \u001b[0m0.9439   \u001b[0m | \u001b[0m1.86e+03 \u001b[0m | \u001b[0m33.42    \u001b[0m | \u001b[0m0.7156   \u001b[0m |\n",
            "| \u001b[0m100      \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m672.2    \u001b[0m | \u001b[0m50.49    \u001b[0m | \u001b[0m0.7159   \u001b[0m |\n",
            "| \u001b[0m101      \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m1.497e+03\u001b[0m | \u001b[0m82.11    \u001b[0m | \u001b[0m0.9601   \u001b[0m |\n",
            "| \u001b[0m102      \u001b[0m | \u001b[0m0.9133   \u001b[0m | \u001b[0m1.431e+03\u001b[0m | \u001b[0m97.69    \u001b[0m | \u001b[0m0.928    \u001b[0m |\n",
            "| \u001b[0m103      \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m1.326e+03\u001b[0m | \u001b[0m36.07    \u001b[0m | \u001b[0m0.5545   \u001b[0m |\n",
            "| \u001b[0m104      \u001b[0m | \u001b[0m0.9082   \u001b[0m | \u001b[0m1.874e+03\u001b[0m | \u001b[0m94.13    \u001b[0m | \u001b[0m0.5007   \u001b[0m |\n",
            "| \u001b[0m105      \u001b[0m | \u001b[0m0.9286   \u001b[0m | \u001b[0m1.857e+03\u001b[0m | \u001b[0m48.0     \u001b[0m | \u001b[0m0.7115   \u001b[0m |\n",
            "=============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 1541, 'n_estimators': 31, 'subsample': 0.7545432080303889}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LGBM = LGBMClassifier(**lgbm_max_params)\n",
        "XGB = XGBClassifier(**xgb_max_params)\n",
        "RF = RandomForestClassifier(**rf_max_params)\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "# VotingClassifier 정의\n",
        "VC = VotingClassifier(estimators=[('rf',RF),('xgb',XGB),('lgbm',LGBM)],voting='soft')"
      ],
      "metadata": {
        "id": "KT9YmXh0rI0P"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VC.fit(Xcc_train,ycc_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OfOFqRgrLHZ",
        "outputId": "03f9b3c7-2923-4d28-a146-4f046eeaf2d3"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('rf',\n",
              "                              RandomForestClassifier(max_depth=71,\n",
              "                                                     n_estimators=64)),\n",
              "                             ('xgb',\n",
              "                              XGBClassifier(eta=0.3423466837492939,\n",
              "                                            gamma=0.3462431050442949,\n",
              "                                            max_depth=1351,\n",
              "                                            subsample=0.7029948366468876)),\n",
              "                             ('lgbm',\n",
              "                              LGBMClassifier(max_depth=1541, n_estimators=31,\n",
              "                                             subsample=0.7545432080303889))],\n",
              "                 voting='soft')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred=VC.predict(test_x.iloc[:,1:])"
      ],
      "metadata": {
        "id": "M6pOEKwfsTbq"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('/content/drive/MyDrive/스마트공장/sample_submission (3).csv')"
      ],
      "metadata": {
        "id": "iQuSwFgszdPN"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit['Y_Class']=pred"
      ],
      "metadata": {
        "id": "qZKRM9IHzjq1"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit['Y_Class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FpN1GzozeU4",
        "outputId": "5c05702c-4d1d-404d-c0fd-54faf9121823"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    265\n",
              "0     32\n",
              "2     13\n",
              "Name: Y_Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submit.to_csv(\"0227_pred2.csv\",index=False)"
      ],
      "metadata": {
        "id": "nmEjfvaSzpfm"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K8blAbNRRM8z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}